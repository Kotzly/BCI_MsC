{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surrounded-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "annotation_desc_dict = {\n",
    "    276: \"Idling EEG (eyes open)\",\n",
    "    277: \"Idling EEG (eyes closed)\",\n",
    "    768: \"Start of a trial\",\n",
    "    769: \"Cue onset left (class 1)\",\n",
    "    770: \"Cue onset right (class 2)\",\n",
    "    771: \"Cue onset foot (class 3)\",\n",
    "    772: \"Cue onset tongue (class 4)\",\n",
    "    783: \"Cue unknown\",\n",
    "    1023: \"Rejected trial\",\n",
    "    1072: \"Eye movements\",\n",
    "    32766: \"Start of a new run\",\n",
    "}\n",
    "\n",
    "annotation_encode_dict = {\n",
    "    276: 0,\n",
    "    277: 1,\n",
    "    768: 2,\n",
    "    769: 3,\n",
    "    770: 4,\n",
    "    771: 5,\n",
    "    772: 6,\n",
    "    783: 7,\n",
    "    1023: 8,\n",
    "    1072: 9,\n",
    "    32766: 10,\n",
    "}\n",
    "\n",
    "def get_annotations(data):\n",
    "    sr = data.info[\"sfreq\"]\n",
    "    n_samples = data._raw_extras[0][\"n_records\"]\n",
    "\n",
    "    onsets = np.trunc(data.annotations.onset * sr).astype(np.uint32, casting=\"unsafe\")\n",
    "    durations = np.trunc(data.annotations.duration * sr).astype(np.uint32, casting=\"unsafe\")\n",
    "    \n",
    "    desc = data.annotations.description.astype(np.uint32)\n",
    "    labels_codes = np.vectorize(annotation_encode_dict.get)(desc)\n",
    "    \n",
    "    n_codes = len(annotation_encode_dict)\n",
    "    labels = np.zeros((n_samples, n_codes))\n",
    "    \n",
    "    for code, onset, duration in zip(labels_codes, onsets, durations):\n",
    "        labels[onset:onset+duration, code] = 1\n",
    "    \n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pretty-wheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.io import read_raw_gdf\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "root = Path(\"C:/Users/paull/Documents/GIT/BCI_MsC/notebooks/BCI_Comp_IV_2a/BCICIV_2a_gdf\")\n",
    "\n",
    "dataset_folder = root\n",
    "mat_files = list(dataset_folder.iterdir())\n",
    "\n",
    "PRELOAD = False\n",
    "\n",
    "def load_gdf_file(filepath):\n",
    "    gdf_data = read_raw_gdf(filepath, preload=PRELOAD)\n",
    "\n",
    "    chs = gdf_data.ch_names\n",
    "\n",
    "    gdf_data = read_raw_gdf(\n",
    "        filepath,\n",
    "        preload=True,\n",
    "        eog=[\"EOG-left\", \"EOG-central\", \"EOG-right\"],\n",
    "        exclude=[x for x in chs if \"EOG\" in x]\n",
    "    )\n",
    "    ch_names = gdf_data.ch_names\n",
    "    info = parse_info(\n",
    "        gdf_data._raw_extras[0][\"subject_info\"]\n",
    "    )\n",
    "    \n",
    "    labels = get_annotations(gdf_data)\n",
    "    \n",
    "    return gdf_data, labels, ch_names, info\n",
    "\n",
    "def parse_info(info_dict):\n",
    "    cols = ['id', 'smoking', 'alcohol_abuse', 'drug_abuse', 'medication', 'weight', 'height', 'sex', 'handedness', 'age']\n",
    "    parsed_info = {k:v for k, v in info_dict.items() if k in cols}\n",
    "    return parsed_info\n",
    "     \n",
    "def load_subject_data(root, subject, mode=None):\n",
    "    if mode is None:\n",
    "        mode = \"train\"\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        filepath = root / f\"{subject}T.gdf\"\n",
    "        gdf_data, labels, ch_names, info = load_gdf_file(filepath)\n",
    "    elif mode == \"test\":\n",
    "        filepath = root / f\"{subject}E.gdf\"\n",
    "        gdf_data, labels, ch_names, info = load_gdf_file(filepath)\n",
    "    elif mode == \"both\":\n",
    "        filepath_t = root / f\"{subject}T.gdf\"\n",
    "        filepath_e = root / f\"{subject}E.gdf\"\n",
    "        gdf_data_t, labels_t, ch_names_t, info_t = load_gdf_file(filepath_t)\n",
    "        gdf_data_e, labels_e, ch_names, info = load_gdf_file(filepath_e)\n",
    "        \n",
    "        \n",
    "        assert np.all(ch_names_t == ch_names)\n",
    "        assert np.all(info_t == info)\n",
    "        \n",
    "        gdf_data = gdf_data_t.copy()\n",
    "        gdf_data._data = np.concatenate(\n",
    "            [\n",
    "                gdf_data_t._data,\n",
    "                gdf_data_e._data,\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        labels = np.concatenate(\n",
    "            [\n",
    "                labels_t,\n",
    "                labels_e\n",
    "            ],\n",
    "            axis=0\n",
    "        )\n",
    "    \n",
    "    return gdf_data, labels, ch_names, info\n",
    "\n",
    "def load_subjects_data(root, datasets=None, mode=\"train\"):\n",
    "    if datasets is None:\n",
    "#         data_dict = {\n",
    "#             \"all\": {\n",
    "#                 filepath.name[:3]: None for filepath in root.glob(\"*T.gdf\")\n",
    "#             }\n",
    "#         }\n",
    "        data_dict = {\n",
    "            filepath.name[:3]: {\n",
    "                filepath.name[:3]: None\n",
    "            } for filepath in root.glob(\"*T.gdf\")\n",
    "        }\n",
    "    else:\n",
    "        data_dict = {\n",
    "            dataset: {\n",
    "                subject_id: {} for subject_id in datasets[dataset]\n",
    "            } for dataset in datasets\n",
    "        }\n",
    "    \n",
    "    chs_ = None\n",
    "    for dataset in data_dict:\n",
    "        for subject_id in data_dict[dataset]:\n",
    "            gdf, labels, chs, info = load_subject_data(root, subject_id, mode=mode)\n",
    "            if chs_ is None:\n",
    "                chs_ = chs\n",
    "            else:\n",
    "                assert chs_ == chs\n",
    "            data_dict[dataset][subject_id] = {\n",
    "                \"gdf\": gdf,\n",
    "                \"chs\": chs,\n",
    "                \"info\": info,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "    \n",
    "    \n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interior-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"train\": [\"A02\", \"A07\", \"A09\", \"A01\"],\n",
    "    \"validation\": [\"A03\", \"A06\"],\n",
    "    \"test\": [\"A04\", \"A05\"],\n",
    "}\n",
    "all_subjects = [f\"A0{i}\" for i in range(10)]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"train\": [\"A02\"],\n",
    "#     \"train\": [\"A02\", \"A07\"],\n",
    "#     \"validation\": [\"A03\"],\n",
    "#     \"test\": [\"A04\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accurate-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoring import mutual_information, coherence, correntropy, apply_pairwise, apply_pairwise_parallel\n",
    "from ica import get_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\paull\\Documents\\GIT\\BCI_MsC\\notebooks\\BCI_Comp_IV_2a\\BCICIV_2a_gdf\\A02T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\Users\\paull\\Documents\\GIT\\BCI_MsC\\notebooks\\BCI_Comp_IV_2a\\BCICIV_2a_gdf\\A02T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 677168  =      0.000 ...  2708.672 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paull\\anaconda3\\envs\\bci\\lib\\site-packages\\mne\\io\\edf\\edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "<ipython-input-2-17f0d3bb7d79>:13: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  gdf_data = read_raw_gdf(filepath, preload=PRELOAD)\n",
      "C:\\Users\\paull\\anaconda3\\envs\\bci\\lib\\site-packages\\mne\\io\\edf\\edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "<ipython-input-2-17f0d3bb7d79>:17: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  gdf_data = read_raw_gdf(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from C:\\Users\\paull\\Documents\\GIT\\BCI_MsC\\notebooks\\BCI_Comp_IV_2a\\BCICIV_2a_gdf\\A02E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from C:\\Users\\paull\\Documents\\GIT\\BCI_MsC\\notebooks\\BCI_Comp_IV_2a\\BCICIV_2a_gdf\\A02E.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 662665  =      0.000 ...  2650.660 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paull\\anaconda3\\envs\\bci\\lib\\site-packages\\mne\\io\\edf\\edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "<ipython-input-2-17f0d3bb7d79>:13: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  gdf_data = read_raw_gdf(filepath, preload=PRELOAD)\n",
      "C:\\Users\\paull\\anaconda3\\envs\\bci\\lib\\site-packages\\mne\\io\\edf\\edf.py:1044: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "<ipython-input-2-17f0d3bb7d79>:17: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  gdf_data = read_raw_gdf(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def join_gdfs(data_dict, datasets_names=None):\n",
    "    new_dict = {}\n",
    "    if datasets_names is None:\n",
    "        datasets_names = data_dict.keys()\n",
    "    \n",
    "    for dataset_name in datasets_names:\n",
    "        all_gdfs = []\n",
    "        all_labels = []\n",
    "        for subject_id in data_dict[dataset_name]:\n",
    "            all_gdfs.append(data_dict[dataset_name][subject_id][\"gdf\"])\n",
    "            all_labels.append(data_dict[dataset_name][subject_id][\"labels\"])\n",
    "\n",
    "        labels = np.concatenate(all_labels, axis=0)\n",
    "        gdf_base = all_gdfs[0].copy()\n",
    "        for gdf in all_gdfs[1:]:\n",
    "            gdf_base._data = np.concatenate(\n",
    "                [\n",
    "                    gdf_base._data,\n",
    "                    gdf._data\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        new_dict[dataset_name] = {\n",
    "            \"all\": {\n",
    "                \"gdf\": gdf_base,\n",
    "                \"labels\": labels,\n",
    "                \"info\": None,\n",
    "                \"chs\": gdf_base.ch_names\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    return new_dict\n",
    "        \n",
    "N_RUNS = 3\n",
    "\n",
    "results = {}\n",
    "\n",
    "fn_dict = {\n",
    "    \"MI\": mutual_information,\n",
    "    \"correntropy\": correntropy,\n",
    "    \"coherence\": coherence\n",
    "}\n",
    "\n",
    "n_components_list = [4, 8, 12, 16, 20, 22]\n",
    "\n",
    "try:\n",
    "    datasets\n",
    "except:\n",
    "    datasets = load_subjects_data(root, datasets=dataset_dict, mode=\"both\")\n",
    "\n",
    "score_calculated_before = {}\n",
    "\n",
    "for n_components in n_components_list:\n",
    "    for ica_method, ica_transform in get_transformers(n_components=n_components).items():\n",
    "        for run_n in range(N_RUNS):\n",
    "            \n",
    "            joined_dataset = join_gdfs(datasets, [\"train\"])\n",
    "\n",
    "            gdf_data = joined_dataset[\"train\"][\"all\"][\"gdf\"]\n",
    "            data = gdf_data.get_data()\n",
    "            ica_transform.fit(data)\n",
    "            \n",
    "            del joined_dataset\n",
    "\n",
    "#             for dataset_name in (\"test\", \"validation\", \"train\"):\n",
    "            for dataset_name in (\"train\",):\n",
    "\n",
    "                for subject_id in datasets[dataset_name]:\n",
    "                    \n",
    "\n",
    "                    gdf_data = datasets[dataset_name][subject_id][\"gdf\"]   \n",
    "                    data = gdf_data.get_data()\n",
    "                    \n",
    "                    data_after = ica_transform.transform(data)\n",
    "\n",
    "                    for fn_name in fn_dict:\n",
    "\n",
    "                        print((fn_name, ica_method, dataset_name, subject_id, run_n, n_components))\n",
    "                        \n",
    "                        if (n_components > 5) or len(data_after) > 2e6:\n",
    "                            apply_fn = apply_pairwise_parallel\n",
    "                        else:\n",
    "                            apply_fn = apply_pairwise\n",
    "                        \n",
    "                        if not (subject_id, fn_name) in score_calculated_before:\n",
    "                            data_before = gdf_data.get_data().T\n",
    "                            score_before = apply_pairwise_parallel(data_before, fn_dict[fn_name])\n",
    "                            score_calculated_before[(subject_id, fn_name)] = score_before\n",
    "\n",
    "                        start = time.time()\n",
    "                        score_after = apply_fn(data_after, fn_dict[fn_name])\n",
    "                        duration = time.time() - start\n",
    "                        \n",
    "                        results[(fn_name, ica_method, dataset_name, subject_id, run_n, n_components)] = {\n",
    "                            \"score_before\": score_calculated_before[(subject_id, fn_name)],\n",
    "                            \"score_after\": score_after,\n",
    "                            \"time\": duration\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = []\n",
    "cols = [\"scoring\", \"algorithm\", \"dataset\", \"subject_id\", \"run\", \"n_components\", \"score_before\", \"score_after\", \"time\"]\n",
    "\n",
    "for k, v in results.items():\n",
    "    df.append(list(k) + list(v.values()))\n",
    "pd.DataFrame(df, columns=cols).to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df, columns=cols)\n",
    "df.groupby([\"scoring\", \"algorithm\", \"dataset\", \"subject_id\", \"n_components\"]).mean().query(\"\"\" (dataset == \"test\") \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "10 + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-citizenship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ica import ICA_METHODS\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ICA_METHODS:\n",
    "    print(method, end=\",\")\n",
    "    x = np.random.rand(10000, 5)\n",
    "    y = method(x)\n",
    "    print(y.shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-hostel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
