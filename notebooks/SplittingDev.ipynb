{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "922240c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ica_benchmark.io.load import Physionet_2009_Dataset, BCI_IV_Comp_Dataset, OpenBMI_Dataset\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "import pandas as pd\n",
    "\n",
    "mne.set_log_level(False)\n",
    "\n",
    "physionet_dataset_folderpath = Path('/home/paulo/Documents/datasets/Physionet')\n",
    "bci_dataset_folderpath = Path('/home/paulo/Documents/datasets/BCI_Comp_IV_2a/gdf/')\n",
    "bci_test_dataset_folderpath = Path('/home/paulo/Documents/datasets/BCI_Comp_IV_2a/true_labels/')\n",
    "openbmi_dataset_folderpath = Path('/home/paulo/Documents/datasets/OpenBMI/edf/')\n",
    "\n",
    "physionet_dataset = Physionet_2009_Dataset(physionet_dataset_folderpath)\n",
    "bci_dataset = BCI_IV_Comp_Dataset(bci_dataset_folderpath, test_folder=bci_test_dataset_folderpath)\n",
    "openbmi_dataset = OpenBMI_Dataset(openbmi_dataset_folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "dfdf8ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 2], [[4], 5]], [5]]\n",
      "[[[1, 2], [[4], 5]], [5]]\n"
     ]
    }
   ],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def flatten_deepest(lst):\n",
    "    res = []\n",
    "    for item in lst:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "            if any(isinstance(i, Iterable) and not isinstance(i, str) for i in item):\n",
    "                res.append(flatten_deepest(item))\n",
    "            else:\n",
    "                res.append(flatten_list(item))\n",
    "        else:\n",
    "            res.append(item)\n",
    "    return res\n",
    "\n",
    "list_of_lists = [\n",
    "    [\n",
    "        [1, 2],\n",
    "        [[4], 5]\n",
    "    ],\n",
    "    [5]\n",
    "]\n",
    "    \n",
    "print(list_of_lists)\n",
    "print(flatten_deepest(list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "c3af492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(\"12\", Iterable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578b9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "e87796cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 10.        ,  14.73684211,  19.47368421,  24.21052632,\n",
       "         28.94736842,  33.68421053,  38.42105263,  43.15789474,\n",
       "         47.89473684,  52.63157895,  57.36842105,  62.10526316,\n",
       "         66.84210526,  71.57894737,  76.31578947,  81.05263158,\n",
       "         85.78947368,  90.52631579,  95.26315789, 100.        ])]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mne import Epochs\n",
    "\n",
    "def make_epochs_splits_indexes(arr, n=None, n_splits=2, sizes=None, shuffle=False, seed=1):\n",
    "    if isinstance(arr, (tuple, list)):\n",
    "        arr = np.array(arr)\n",
    "    np.random.seed(seed)\n",
    "    if n is None:\n",
    "        if isinstance(arr, Epochs):\n",
    "            n = len(arr.events)\n",
    "        else:\n",
    "            n = len(arr)\n",
    "    sizes = sizes or [1 / n_splits] * n_splits\n",
    "\n",
    "    assert np.sum(sizes) == 1.\n",
    "    \n",
    "    sizes = np.cumsum(\n",
    "        [0] + [int(size * n) for size in sizes]\n",
    "    )\n",
    "    \n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    slices = [slice(start, end) for start, end in zip(sizes[:-1], sizes[1:])]\n",
    "    indexes = [idx[s] for s in slices]\n",
    "    return indexes\n",
    "\n",
    "def make_epochs_splits(arr, n=None, n_splits=2, sizes=None, shuffle=False, seed=1):\n",
    "    indexes = make_epochs_splits_indexes(arr, n=n, n_splits=n_splits, sizes=sizes, shuffle=shuffle, seed=seed)\n",
    "    arrs = [arr[idx] for idx in indexes]\n",
    "    return arrs\n",
    "\n",
    "make_epochs_splits(np.linspace(10, 100, 20), sizes=[1.], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "f338fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.        ,  14.73684211,  19.47368421,  24.21052632,\n",
       "        28.94736842,  33.68421053,  38.42105263,  43.15789474,\n",
       "        47.89473684,  52.63157895,  57.36842105,  62.10526316,\n",
       "        66.84210526,  71.57894737,  76.31578947,  81.05263158,\n",
       "        85.78947368,  90.52631579,  95.26315789, 100.        ])"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(10, 100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e4953d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8531/4001071399.py\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m#     fold_sizes=[.4, .6]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FOLD {j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/4001071399.py\u001b[0m in \u001b[0;36mmake_splits\u001b[0;34m(self, inter_session, inter_subject)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintra_session_default_splitting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You are using the intra session protocol but passed more than 1 session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/4001071399.py\u001b[0m in \u001b[0;36mintra_session_default_splitting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Your are using an intra session splitting but had more than 1 session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         splits = [\n\u001b[0m\u001b[1;32m     70\u001b[0m             (\n\u001b[1;32m     71\u001b[0m                 \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/4001071399.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m             (\n\u001b[1;32m     71\u001b[0m                 \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uid' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Splitter():\n",
    "    \n",
    "    INTER_SESSION = True\n",
    "    INTER_SUBJECT = True\n",
    "    TRAIN_TEST = True\n",
    "    \n",
    "    UNIQUE_SESSION = False\n",
    "    \n",
    "    SESSION_KWARGS = dict(intra=dict(), inter=dict())\n",
    "    \n",
    "    def __init__(self, dataset, uids, sessions, train_folds, load_kwargs=None, fold_sizes=None):\n",
    "        self.dataset = dataset\n",
    "        self.uids = uids\n",
    "        self.sessions = sessions\n",
    "        self.train_folds = train_folds\n",
    "        self.load_kwargs = load_kwargs or load_kwargs\n",
    "        self.fold_sizes = fold_sizes\n",
    "\n",
    "    def inter_subject_splitting(self, shuffle=False, sizes=[.5, .5], seed=1):\n",
    "        np.random.seed(seed)\n",
    "        uids = deepcopy(self.uids)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(uids)\n",
    "        splits_uids = make_epochs_splits(uids, sizes=sizes)\n",
    "        # [(info, epochs), (info, epochs), ...]\n",
    "        splits = [\n",
    "            (\n",
    "                dict(\n",
    "                    sessions=self.sessions,\n",
    "                    train_folds=self.train_folds,\n",
    "                    uid=split_uids\n",
    "                ),\n",
    "                mne.concatenate_epochs(\n",
    "                    [\n",
    "                        self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                        for session in self.sessions\n",
    "                        for train in self.train_folds\n",
    "                        for uid in split_uids\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            for split_uids in splits_uids\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def inter_session_splitting(self):\n",
    "        assert len(self.sessions) > 1, \"You are using the inter session protocol, but only passed 1 session\"\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uids=self.uids, sessions=[session], train_folds=self.train_folds),\n",
    "                mne.concatenate_epochs(\n",
    "                    [\n",
    "                        self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                        for train\n",
    "                        in self.train_folds\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            for uid in self.uids\n",
    "            for session in self.sessions\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def intra_session_default_splitting(self):\n",
    "        \n",
    "        assert len(self.sessions) == 1, \"Your are using an intra session splitting but had more than 1 session\"\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uid=self.uids, sessions=[session], train_folds=[train]),\n",
    "                self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "            )\n",
    "            for session in self.sessions\n",
    "            for train in self.train_folds\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def intra_session_splitting(self, uid, session):\n",
    "        epochs = mne.concatenate_epochs(\n",
    "            [\n",
    "                self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                for train in self.train_folds\n",
    "            ]\n",
    "        )\n",
    "        splits = make_epochs_splits(\n",
    "            epochs,\n",
    "            n=len(epochs.events),\n",
    "            sizes=self.fold_sizes\n",
    "        )\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uid=self.uids, session=session, train_folds=self.train_folds, size=size),\n",
    "                split\n",
    "            )\n",
    "            for split, size\n",
    "            in zip(splits, self.fold_sizes)\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def make_splits(self, inter_session=True, inter_subject=False):\n",
    "\n",
    "        assert all(np.isin(self.uids, self.dataset.list_uids()))\n",
    "        session_key = \"inter\" if inter_session else \"intra\"\n",
    "        subject_key = \"inter\" if inter_subject else \"intra\"\n",
    "        \n",
    "        splits = list()\n",
    "\n",
    "        # Inter session for the inter subject protocol does not make sense.\n",
    "        # So, if inter_subject is true, inter_session does not matter\n",
    "        if inter_subject:\n",
    "            splits = self.inter_subject_splitting()\n",
    "            yield splits\n",
    "            \n",
    "        # Intra subject\n",
    "        else:\n",
    "            \n",
    "            for uid in self.uids:\n",
    "                if inter_session:\n",
    "                    splits = self.inter_session_splitting()\n",
    "                    yield splits\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    if self.fold_sizes is None:\n",
    "                        splits = self.intra_session_default_splitting()\n",
    "                    else:\n",
    "                        assert len(self.sessions) == 1, \"You are using the intra session protocol but passed more than 1 session\"\n",
    "                        session = self.sessions[0]\n",
    "                        splits = self.intra_session_splitting(uid, session)\n",
    "                        \n",
    "                    yield splits\n",
    "import numpy as np\n",
    "kwargs = dict(\n",
    "    inter_session=False,\n",
    "    inter_subject=False\n",
    ")\n",
    "splitter = Splitter(\n",
    "    openbmi_dataset,\n",
    "    uids=[\"1\", \"2\"],\n",
    "    sessions=[1],\n",
    "    train_folds=[True, False],\n",
    "    load_kwargs=dict(\n",
    "        reject=False\n",
    "    )\n",
    "#     fold_sizes=[.4, .6]\n",
    ")\n",
    "for j, splits in enumerate(splitter.make_splits(**kwargs)):\n",
    "    print(f\"FOLD {j}\")\n",
    "    for i, (info, split) in enumerate(splits):\n",
    "        print(f\"Split {i}\")\n",
    "        print(\"\\tInfo\", info)\n",
    "        print(\"\\tSplit\", split)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ccd73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '1', 'train': True, 'session': 1}, {'uid': '1', 'train': True, 'session': 2}]\n",
      "\n",
      "[{'uid': '1', 'train': False, 'session': 1}, {'uid': '1', 'train': False, 'session': 2}]\n",
      "\n",
      "[{'uid': '2', 'train': True, 'session': 1}, {'uid': '2', 'train': True, 'session': 2}]\n",
      "\n",
      "[{'uid': '2', 'train': False, 'session': 1}, {'uid': '2', 'train': False, 'session': 2}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "class SplitArg():\n",
    "    def __init__(self, name, arg_list, to_split):\n",
    "        self.name = name\n",
    "        self.arg_list = arg_list\n",
    "        self.to_split = to_split\n",
    "        \n",
    "\n",
    "def group_iterator(*args):\n",
    "    split_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in args\n",
    "        if arg.to_split\n",
    "    ]\n",
    "    merge_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in args\n",
    "        if not arg.to_split\n",
    "    ]\n",
    "    # Split loop\n",
    "    for split_args in product(*[arg.arg_list for arg in split_args_iter]):\n",
    "                \n",
    "        split_arg_dict = {\n",
    "            arg.name: split_args[i]\n",
    "            for i, arg\n",
    "            in enumerate(split_args_iter)\n",
    "        }\n",
    "        # Merge loop\n",
    "        merge_args_list = list()\n",
    "        for merge_args in product(*[arg.arg_list for arg in merge_args_iter]):\n",
    "            merge_arg_dict = {\n",
    "                arg.name: merge_args[i]\n",
    "                for i, arg\n",
    "                in enumerate(merge_args_iter)\n",
    "            }\n",
    "            run_kwargs = {**split_arg_dict, **merge_arg_dict}\n",
    "            merge_args_list.append(run_kwargs)\n",
    "        yield merge_args_list\n",
    "\n",
    "\n",
    "load_kwargs=dict(\n",
    "    tmin=1,\n",
    "    tmax=3.5,\n",
    "    reject=False\n",
    ")\n",
    "\n",
    "from copy import deepcopy\n",
    "def split(dataset, uids, sessions, train_folds, fold_sizes, load_kwargs):\n",
    "    kwargs_iterator = group_iterator(\n",
    "        SplitArg(\"uid\", uids, True),\n",
    "        SplitArg(\"session\", sessions, False),\n",
    "        SplitArg(\"train\", train_folds, True),\n",
    "    )\n",
    "    epochs_list = list()\n",
    "    \n",
    "    for splits_kwargs in kwargs_iterator:\n",
    "        split_epochs = list()\n",
    "#         for split_kwargs in splits_kwargs:\n",
    "#             kwargs = deepcopy(split_kwargs)\n",
    "#             uid = kwargs.pop(\"uid\")\n",
    "#             epochs, _ = dataset.load_subject(uid, **kwargs, **load_kwargs)\n",
    "#             split_epochs.append(epochs)\n",
    "        print(splits_kwargs)\n",
    "        print()\n",
    "#         if fold_sizes is not None:\n",
    "#             split_epochs = mne.concatenate_epochs(split_epochs)\n",
    "#             split_epochs = make_epochs_splits(split_epochs, n=len(split_epochs.events), sizes=fold_sizes)\n",
    "#         yield split_epochs\n",
    "        yield splits_kwargs\n",
    "\n",
    "#         epochs_list.append(epochs)\n",
    "#     epochs = mne.concatenate_epochs(epochs_list)\n",
    "#     yield split\n",
    "for k in split(openbmi_dataset, [\"1\", \"2\"], [1, 2], [True, False], None, load_kwargs):\n",
    "#     print(k)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b803c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, ['a', 'b'])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136302/3208119849.py\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miter_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0munpack_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_136302/3208119849.py\u001b[0m in \u001b[0;36munpack_product\u001b[0;34m(iters)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mis_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0miter_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miter_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0munpack_product\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "iters = [\n",
    "    [1, 2, 3],\n",
    "    [\"a\", \"b\", \"c\", \"d\"]\n",
    "]\n",
    "(1, \"a\"), (1, \"b\"), (1, \"c\"), (1, \"d\")\n",
    "\n",
    "iters = [\n",
    "    [1, 2, 3],\n",
    "    [\n",
    "        [\"a\", \"b\"], [\"A\", \"B\"]\n",
    "    ],\n",
    "    [True, False]\n",
    "]\n",
    "(1, \"a\"), (1, \"b\"), (1, \"c\"), (1, \"d\")\n",
    "\n",
    "\n",
    "iters = [\n",
    "    [1, 2, 3],\n",
    "    [\n",
    "        [\"a\", \"b\"], [\"A\", \"B\"]\n",
    "    ]\n",
    "]\n",
    "(1, \"a\", \"A\"), (1, \"a\", \"B\"), (1, \"b\", \"A\"), (1, \"b\", \"B\")\n",
    "\n",
    "def unpack_product(iters):\n",
    "    iters = product(*iters)\n",
    "    for iter_list in iters:\n",
    "        print(iter_list)\n",
    "        is_list = False\n",
    "        for i, iter_value in enumerate(iter_list):\n",
    "            is_list = is_list or isinstance(iter_value, list)\n",
    "        for i, iter_value in enumerate(iter_list):\n",
    "            iter_list[i] = [iter_list[i]] if is_list else iter_list[i]\n",
    "    return iter_list\n",
    "unpack_product(iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d0d0e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[0]\n",
      "\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[1]\n",
      "\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[2]\n",
      "\n",
      "[ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[3]\n",
      "\n",
      "[ 0  1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[4]\n",
      "\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[5]\n",
      "\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[6]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[7]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[8]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[9]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[10]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[11]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[12]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[13]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[14]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[15]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[16]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[17]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[18]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[19]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[20]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[21]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[22]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[23]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[24]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[25]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[26]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[27]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[28]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[29]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[30]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[31]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[32]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[33]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[34]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[35]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[36]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[37]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[38]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[39]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[40]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[41]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[42]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[43]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[44]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[45]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[46]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[47]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[48]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[49]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[50]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[51]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[52]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[53]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[54]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[55]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[56]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[57]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[58]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[59]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[60]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[61]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[62]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[63]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[64]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[65]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[66]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[67]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[68]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[69]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[70]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[71]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[72]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[73]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[74]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[75]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[76]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[77]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[78]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[79]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[80]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[81]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[82]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[83]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[84]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[85]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[86]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[87]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 89 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[88]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 90 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[89]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 91 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[90]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 92 93 94 95 96\n",
      " 97 98 99]\n",
      "[91]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 93 94 95 96\n",
      " 97 98 99]\n",
      "[92]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 94 95 96\n",
      " 97 98 99]\n",
      "[93]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 95 96\n",
      " 97 98 99]\n",
      "[94]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 96\n",
      " 97 98 99]\n",
      "[95]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 97 98 99]\n",
      "[96]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 98 99]\n",
      "[97]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 99]\n",
      "[98]\n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98]\n",
      "[99]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "\n",
    "cv = LeaveOneOut()\n",
    "for a, b in cv.split(np.arange(100)):\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "17ecbc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/2685822741.py:27: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split({'uid': '25', 'session': 1, 'run': 1},{'uid': '25', 'session': 1, 'run': 2})]\n",
      "[<Epochs |  60 events (all good), 1 - 3.5 sec, baseline off, ~71.0 MB, data loaded,\n",
      " '0': 24\n",
      " '1': 36>, <Epochs |  140 events (all good), 1 - 3.5 sec, baseline off, ~165.7 MB, data loaded,\n",
      " '0': 76\n",
      " '1': 64>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/2685822741.py:27: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split({'uid': '25', 'session': 2, 'run': 1},{'uid': '25', 'session': 2, 'run': 2})]\n",
      "[<Epochs |  60 events (all good), 1 - 3.5 sec, baseline off, ~71.0 MB, data loaded,\n",
      " '0': 24\n",
      " '1': 36>, <Epochs |  140 events (all good), 1 - 3.5 sec, baseline off, ~165.7 MB, data loaded,\n",
      " '0': 76\n",
      " '1': 64>]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, LeaveOneOut\n",
    "from itertools import tee\n",
    "from warnings import warn\n",
    "\n",
    "def remove_key(d, k):\n",
    "    return {\n",
    "        key: value\n",
    "        for key, value in d.items()\n",
    "        if key != k\n",
    "    }\n",
    "\n",
    "class Split():\n",
    "    \n",
    "    def __init__(self, kwarg_dict_list, fold_sizes=None):\n",
    "        self.kwargs_list = kwarg_dict_list\n",
    "        self.fold_sizes = fold_sizes\n",
    "    \n",
    "    def to_dataframe():\n",
    "        return pd.DataFrame.from_records(self.kwargs_list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        dict_reps = [str(d) for d in self.kwargs_list]\n",
    "        return \"Split({})\".format(\",\".join(dict_reps))\n",
    "    \n",
    "    def load_epochs(self, dataset, **load_kwargs):\n",
    "        epochs = mne.concatenate_epochs(\n",
    "            [\n",
    "                dataset.load_subject(kwargs[\"uid\"], **remove_key(kwargs, \"uid\"), **load_kwargs)[0]\n",
    "                for kwargs in self.kwargs_list\n",
    "            ]\n",
    "        )\n",
    "        return epochs\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(product(openbmi_dataset.list_uids(), [\"1\", \"2\"], [True, False]), columns=[\"uid\", \"session\", \"train\"])\n",
    "\n",
    "# k = [\"uid\", \"session\"]\n",
    "\n",
    "uids = openbmi_dataset.list_uids()[:3]\n",
    "sessions = [1, 2]\n",
    "runs = [1, 2]\n",
    "\n",
    "\n",
    "# for uid, session in product(uids, sessions):\n",
    "#     a = df.query(\"uid == @uid\").query(\"session == @session\")\n",
    "#     display(a)\n",
    "                \n",
    "class Splitter():\n",
    "    \n",
    "    INTER_SESSION = True\n",
    "    INTER_SUBJECT = True\n",
    "    TRAIN_TEST = True\n",
    "    \n",
    "    UNIQUE_SESSION = False\n",
    "    \n",
    "    SESSION_KWARGS = dict(intra=dict(), inter=dict())\n",
    "    \n",
    "    def default_cv_splitter(self):\n",
    "        return KFold(4)\n",
    "    \n",
    "    def __init__(self, dataset, uids, sessions, runs, load_kwargs=None, cv_splitter=None, unsafe=False, intra_session_shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.uids = uids\n",
    "        self.sessions = sessions\n",
    "        self.runs = runs\n",
    "        self.load_kwargs = load_kwargs or load_kwargs\n",
    "        self.cv_splitter = cv_splitter or self.default_cv_splitter()\n",
    "        self.intra_session_shuffle = intra_session_shuffle\n",
    "        \n",
    "    def validate_config(self, mode, fold_sizes=None):\n",
    "        valid_modes = [\n",
    "            \"inter_subject\",\n",
    "            \"inter_session\",\n",
    "            \"intra_session_intra_run\",\n",
    "            \"intra_session_intra_run_merge\",\n",
    "            \"intra_session_inter_run\"\n",
    "        ]\n",
    "        \n",
    "        assert mode in valid_modes, \"Please choose one mode among the following: {}\".format(\", \".join(valid_modes))\n",
    "        if mode == \"inter_subject\":\n",
    "            if fold_sizes is not None:\n",
    "                warn(\"You are using the inter_subject mode, so the fold_sizes argument will not be used\")\n",
    "#         if mode == \"intra_session\":\n",
    "#             if len(self.runs) > \n",
    "        elif mode == \"inter_session\":\n",
    "            if len(self.runs) > 1:\n",
    "                warn(\"You are using inter session protocol with more than one run. All runs from each session will be concatenated and yielded in different steps.\")\n",
    "\n",
    "        elif (mode in (\"intra_session_intra_run\", \"intra_session_intra_run_merge\")):\n",
    "            if fold_sizes is None:\n",
    "                warn(\"You are using intra session protocol with no fold sizes. The splitter will only yield one epoch at time\")\n",
    "\n",
    "        elif (mode == \"intra_session_inter_run\"):\n",
    "            if (len(self.runs) == 1):\n",
    "                warn(\"You are using an intra session protocol, splitting by run, but only passed one run. The splitter can only yield one epoch at time (from the only run you passed as argument)\")\n",
    "\n",
    "    def inter_subject(self):\n",
    "        for uid_splits_idxs in self.cv_splitter.split(self.uids):\n",
    "            splits_uids = [uids[idx] for idx in uid_splits_idxs]\n",
    "            yield [\n",
    "                Split(\n",
    "                    [\n",
    "                        dict(\n",
    "                            uid=uid,\n",
    "                            session=session,\n",
    "                            run=run\n",
    "                        )\n",
    "                        for uid in split_uids\n",
    "                        for session in self.sessions\n",
    "                        for run in self.runs\n",
    "                    ]\n",
    "                )\n",
    "                for split_uids in splits_uids\n",
    "            ]\n",
    "\n",
    "    def inter_session(self):\n",
    "        for uid in self.uids:\n",
    "            yield [\n",
    "                Split(\n",
    "                    [\n",
    "                        dict(\n",
    "                            uid=uid,\n",
    "                            session=session,\n",
    "                            run=run\n",
    "                        )\n",
    "                        for run in self.runs\n",
    "                    ]\n",
    "                )\n",
    "                for session in self.sessions\n",
    "            ]\n",
    "\n",
    "    def intra_session_inter_run(self):\n",
    "        for uid in self.uids:\n",
    "            for session in self.sessions:\n",
    "                yield [\n",
    "                    Split(\n",
    "                        [\n",
    "                            dict(\n",
    "                                uid=uid,\n",
    "                                session=session,\n",
    "                                run=run\n",
    "                            )\n",
    "                        ]\n",
    "                    )\n",
    "                    for run in self.runs\n",
    "                ]\n",
    "    \n",
    "    def intra_session_intra_run(self):\n",
    "        for uid in self.uids:\n",
    "            for session in self.sessions:\n",
    "                for run in self.runs:\n",
    "                    yield [\n",
    "                        Split(\n",
    "                            [\n",
    "                                dict(\n",
    "                                    uid=uid,\n",
    "                                    session=session,\n",
    "                                    run=run\n",
    "                                )\n",
    "                            ]\n",
    "                        )\n",
    "                    ]\n",
    "\n",
    "    def intra_session_intra_run_merge(self):\n",
    "        for uid in self.uids:\n",
    "            for session in self.sessions:\n",
    "                yield [\n",
    "                    Split(\n",
    "                        [\n",
    "                            dict(\n",
    "                                uid=uid,\n",
    "                                session=session,\n",
    "                                run=run\n",
    "                            )\n",
    "                            for run in self.runs\n",
    "                        ]\n",
    "                    )\n",
    "                ]\n",
    "                    \n",
    "    def yield_splits_epochs(self, mode, fold_sizes=None):\n",
    "        self.validate_config(mode, fold_sizes)\n",
    "\n",
    "        split_fn_dict = dict(\n",
    "            # Intra subject, inter session\n",
    "            inter_session=self.inter_session,\n",
    "            # Inter subject, will concatenate all sessions and runs\n",
    "            inter_subject=self.inter_subject,\n",
    "            # Intra subject, intra_session, inter run (will split runs)\n",
    "            intra_session_inter_run=self.intra_session_inter_run,\n",
    "            # Intra subject, intra_session, intra run (will split using fold sizes)\n",
    "            intra_session_intra_run=self.intra_session_intra_run,\n",
    "            # Intra subject, intra_session, intra run (will merge all runs and split using fold sizes)\n",
    "            intra_session_intra_run_merge=self.intra_session_intra_run_merge,\n",
    "        )\n",
    "\n",
    "        split_fn = split_fn_dict[mode]\n",
    "        for splits in split_fn():\n",
    "            splits_epochs = [\n",
    "                split.load_epochs(openbmi_dataset, **load_kwargs)\n",
    "                for split in splits\n",
    "            ]\n",
    "            assert len(splits) == len(splits_epochs)\n",
    "            if mode in (\"intra_session_intra_run\", \"intra_session_intra_run_merge\") and (fold_sizes is not None):\n",
    "                assert len(splits_epochs) == 1\n",
    "                epochs = splits_epochs[0]\n",
    "                splits_epochs = make_epochs_splits(epochs, sizes=fold_sizes, shuffle=self.intra_session_shuffle)\n",
    "            yield splits, splits_epochs\n",
    "                    \n",
    "splitter = Splitter(\n",
    "    openbmi_dataset,\n",
    "    uids=openbmi_dataset.list_uids()[:1],\n",
    "    sessions=openbmi_dataset.SESSIONS,\n",
    "#     runs=openbmi_dataset.RUNS,\n",
    "    runs=[1, 2],\n",
    "    load_kwargs=dict(\n",
    "        reject=False\n",
    "    ),\n",
    "#     fold_sizes=None,\n",
    "    cv_splitter=None,\n",
    "    intra_session_shuffle=False\n",
    ")\n",
    "for splits, epochs in splitter.yield_splits_epochs(mode=\"intra_session_intra_run_merge\", fold_sizes=[.3, .7]):\n",
    "    print(splits)\n",
    "    print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0cdba04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'l1': 1, 'l2': 'a'},\n",
       " {'l1': 1, 'l2': 'b'},\n",
       " {'l1': 1, 'l2': 'c'},\n",
       " {'l1': 2, 'l2': 'a'},\n",
       " {'l1': 2, 'l2': 'b'},\n",
       " {'l1': 2, 'l2': 'c'},\n",
       " {'l1': 3, 'l2': 'a'},\n",
       " {'l1': 3, 'l2': 'b'},\n",
       " {'l1': 3, 'l2': 'c'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_product(*inp):\n",
    "    return (dict(zip(inp.keys(), values)) for values in product(*inp.values()))\n",
    "\n",
    "\n",
    "list(product_dict(l1=l1, l2=l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "396ada07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Split({'l1': 1, 'l3': 'a'}), Split({'l1': 1, 'l3': 'b'})]\n",
      "[Split({'l1': 2, 'l3': 'a'}), Split({'l1': 2, 'l3': 'b'})]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2]\n",
    "l2 = [101, 102]\n",
    "l3 = [\"a\", \"b\"]\n",
    "l4 = [\"A\", \"B\"]\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "def _split_group_iterator(outer_split_kwargs=None, inner_split_kwargs=None, merge_kwargs=None):\n",
    "    outer_split_kwargs = outer_split_kwargs or dict()\n",
    "    inner_split_kwargs = inner_split_kwargs or dict()\n",
    "    merge_kwargs = merge_kwargs or dict()\n",
    "\n",
    "    for outside_kwargs in product_dict(**outer_split_kwargs):\n",
    "        splits = [\n",
    "            [\n",
    "                dict(\n",
    "                    **outside_kwargs,\n",
    "                    **inside_kwargs,\n",
    "                    **merge_kwargs\n",
    "                )\n",
    "                for merge_kwargs in product_dict(**merge_kwargs)\n",
    "            ]\n",
    "            for inside_kwargs in product_dict(**inner_split_kwargs)\n",
    "        ]\n",
    "        yield splits\n",
    "\n",
    "def split_group_iterator(outer_split_kwargs=None, inner_split_kwargs=None, merge_kwargs=None):\n",
    "\n",
    "    for iteration_kwargs_list in _split_group_iterator(outer_split_kwargs, inner_split_kwargs, merge_kwargs):\n",
    "        splits = [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **kwargs\n",
    "                    )\n",
    "                    for kwargs in kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for kwargs_list in iteration_kwargs_list\n",
    "        ]\n",
    "        yield splits\n",
    "        \n",
    "# def split_group_iterator(outer_kwargs_list_dict=None, inner_kwargs_list_dict=None):\n",
    "#     outer_kwargs_list_dict = outer_kwargs_list_dict or dict()\n",
    "#     inner_kwargs_list_dict = inner_kwargs_list_dict or dict()    \n",
    "\n",
    "#     for kwargs in group_iterator(outer_kwargs_list_dict, inner_kwargs_list_dict):\n",
    "# #         yield = [\n",
    "# #             Split(kwargs)\n",
    "# #         ]\n",
    "#         yield kwargs\n",
    "        \n",
    "# a = group_iterator(dict(l1=l1, l2=l2), dict(l3=l3))\n",
    "a = split_group_iterator(dict(l=l1), dict(l3=l3), dict())\n",
    "a = list(a)\n",
    "for x in a:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e31f05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function Recurse (y, number) \n",
    "#    if (number > 1)\n",
    "#       Recurse ( y, number - 1 )\n",
    "#    else\n",
    "#       for x in range (y)\n",
    "#           whatever()\n",
    "\n",
    "l1 = [1, 2]\n",
    "l2 = [101, 102]\n",
    "l3 = [\"a\", \"b\"]\n",
    "l4 = [\"A\", \"B\"]\n",
    "\n",
    "def group_iterator(split_kwargs_dicts, d=None, level=None):\n",
    "    \n",
    "    level = level or 0\n",
    "#     level = level or len(split_kwargs_dicts) - 1\n",
    "    d = d or dict()\n",
    "\n",
    "\n",
    "    for inner_d in product_dict(**split_kwargs_dicts[level]): \n",
    "\n",
    "#         if level == 0:\n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "\n",
    "            yield {**d, **inner_d}\n",
    "        else:\n",
    "\n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "            yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "71cdea0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'session': 1, 'uid': 1},\n",
       "  {'session': 1, 'uid': 2},\n",
       "  {'session': 1, 'uid': 3},\n",
       "  {'session': 1, 'uid': 4}],\n",
       " [{'session': 2, 'uid': 1},\n",
       "  {'session': 2, 'uid': 2},\n",
       "  {'session': 2, 'uid': 3},\n",
       "  {'session': 2, 'uid': 4}]]"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two level iterator, first level is cross product of l1 and l2, inside level is the possibilities of l3\n",
    "a = group_iterator([dict(session=[1, 2]), dict(uid=[1, 2, 3, 4])])\n",
    "a = list(a)\n",
    "b = [list(x) for x in a]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252ecd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'l1': 1, 'l2': 1},\n",
       "  {'l1': 1, 'l2': 2},\n",
       "  {'l1': 1, 'l2': 3},\n",
       "  {'l1': 1, 'l2': 4}],\n",
       " [{'l1': 2, 'l2': 1},\n",
       "  {'l1': 2, 'l2': 2},\n",
       "  {'l1': 2, 'l2': 3},\n",
       "  {'l1': 2, 'l2': 4}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[{'l1': 1, 'l2': 1},\n",
    "  {'l1': 1, 'l2': 2},\n",
    "  {'l1': 1, 'l2': 3},\n",
    "  {'l1': 1, 'l2': 4}],\n",
    " [{'l1': 2, 'l2': 1},\n",
    "  {'l1': 2, 'l2': 2},\n",
    "  {'l1': 2, 'l2': 3},\n",
    "  {'l1': 2, 'l2': 4}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b23fd108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'l1': 1, 'l2': 101, 'l3': 'a'},\n",
       "  {'l1': 1, 'l2': 101, 'l3': 'b'},\n",
       "  {'l1': 1, 'l2': 102, 'l3': 'a'},\n",
       "  {'l1': 1, 'l2': 102, 'l3': 'b'}],\n",
       " [{'l1': 2, 'l2': 101, 'l3': 'a'},\n",
       "  {'l1': 2, 'l2': 101, 'l3': 'b'},\n",
       "  {'l1': 2, 'l2': 102, 'l3': 'a'},\n",
       "  {'l1': 2, 'l2': 102, 'l3': 'b'}]]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two level iterator, first level varies possibilities for l1, inner level is the cross product of l2 and l3\n",
    "a = group_iterator([dict(l1=l1), dict(l2=l2, l3=l3)])\n",
    "a = list(a)\n",
    "b = [list(x) for x in a]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "22cb4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 1, 'l2': 101, 'l3': 'a'}\n",
      "{'l1': 1, 'l2': 101, 'l3': 'b'}\n",
      "{'l1': 1, 'l2': 102, 'l3': 'a'}\n",
      "{'l1': 1, 'l2': 102, 'l3': 'b'}\n",
      "{'l1': 2, 'l2': 101, 'l3': 'a'}\n",
      "{'l1': 2, 'l2': 101, 'l3': 'b'}\n",
      "{'l1': 2, 'l2': 102, 'l3': 'a'}\n",
      "{'l1': 2, 'l2': 102, 'l3': 'b'}\n"
     ]
    }
   ],
   "source": [
    "# 3 level iterator\n",
    "a = group_iterator(\n",
    "    [\n",
    "        dict(l1=l1),\n",
    "        dict(l2=l2),\n",
    "        dict(l3=l3)\n",
    "    ]\n",
    ")\n",
    "for level_1_iter in a:\n",
    "    for level_2_iter in level_1_iter:\n",
    "        for level_3_iter in level_2_iter:\n",
    "            print(level_3_iter)\n",
    "# b = [list(x) for x in a]\n",
    "# c = [list(x) for x in b]\n",
    "# d = [list(x) for x in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "05fa0cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 1, 'l21': 101, 'l22': 101, 'l3': 'a'}\n",
      "{'l1': 1, 'l21': 101, 'l22': 101, 'l3': 'b'}\n",
      "{'l1': 1, 'l21': 101, 'l22': 102, 'l3': 'a'}\n",
      "{'l1': 1, 'l21': 101, 'l22': 102, 'l3': 'b'}\n",
      "{'l1': 1, 'l21': 102, 'l22': 101, 'l3': 'a'}\n",
      "{'l1': 1, 'l21': 102, 'l22': 101, 'l3': 'b'}\n",
      "{'l1': 1, 'l21': 102, 'l22': 102, 'l3': 'a'}\n",
      "{'l1': 1, 'l21': 102, 'l22': 102, 'l3': 'b'}\n",
      "{'l1': 2, 'l21': 101, 'l22': 101, 'l3': 'a'}\n",
      "{'l1': 2, 'l21': 101, 'l22': 101, 'l3': 'b'}\n",
      "{'l1': 2, 'l21': 101, 'l22': 102, 'l3': 'a'}\n",
      "{'l1': 2, 'l21': 101, 'l22': 102, 'l3': 'b'}\n",
      "{'l1': 2, 'l21': 102, 'l22': 101, 'l3': 'a'}\n",
      "{'l1': 2, 'l21': 102, 'l22': 101, 'l3': 'b'}\n",
      "{'l1': 2, 'l21': 102, 'l22': 102, 'l3': 'a'}\n",
      "{'l1': 2, 'l21': 102, 'l22': 102, 'l3': 'b'}\n"
     ]
    }
   ],
   "source": [
    "# 3 level iterator\n",
    "a = group_iterator(\n",
    "    [\n",
    "        dict(l1=l1),\n",
    "        dict(l21=l2, l22=l2),\n",
    "        dict(l3=l3)\n",
    "    ]\n",
    ")\n",
    "for level_1_iter in a:\n",
    "    for level_2_iter in level_1_iter:\n",
    "        for level_3_iter in level_2_iter:\n",
    "            print(level_3_iter)\n",
    "# b = [list(x) for x in a]\n",
    "# c = [list(x) for x in b]\n",
    "# d = [list(x) for x in c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "827222f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 1, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 1, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 1, 'l4': 2, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 1, 'l5': 2, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 1, 'l6': 2}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 1}\n",
      "{'l1': 2, 'l2': 2, 'l3': 2, 'l4': 2, 'l5': 2, 'l6': 2}\n"
     ]
    }
   ],
   "source": [
    "# Indefinitely deep iterator\n",
    "a = group_iterator(\n",
    "    [\n",
    "        dict(l1=[1, 2]),\n",
    "        dict(l2=[1, 2]),\n",
    "        dict(l3=[1, 2]),\n",
    "        dict(l4=[1, 2]),\n",
    "        dict(l5=[1, 2]),\n",
    "        dict(l6=[1, 2]),        \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "for level_1_iter in a:\n",
    "    for level_2_iter in level_1_iter:\n",
    "        for level_3_iter in level_2_iter:\n",
    "            for level_4_iter in level_3_iter:\n",
    "                for level_5_iter in level_4_iter:\n",
    "                    for level_6_iter in level_5_iter:\n",
    "                        print(level_6_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "f65b258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter, Split, Merge_idx\n",
      "0 0 0 {'uid': '1', 'session': 1, 'run': 1}\n",
      "0 0 1 {'uid': '1', 'session': 1, 'run': 2}\n",
      "0 1 0 {'uid': '1', 'session': 2, 'run': 1}\n",
      "0 1 1 {'uid': '1', 'session': 2, 'run': 2}\n",
      "1 0 0 {'uid': '2', 'session': 1, 'run': 1}\n",
      "1 0 1 {'uid': '2', 'session': 1, 'run': 2}\n",
      "1 1 0 {'uid': '2', 'session': 2, 'run': 1}\n",
      "1 1 1 {'uid': '2', 'session': 2, 'run': 2}\n"
     ]
    }
   ],
   "source": [
    "iterable = group_iterator(\n",
    "    [\n",
    "        dict(uid=[\"1\", \"2\"]),\n",
    "        dict(session=[1, 2]),\n",
    "        dict(run=[1, 2])\n",
    "    ]\n",
    ")\n",
    "# a = list(iterable)\n",
    "# c = [[list(x) for x in b] for b in a]\n",
    "print(\"Iter, Split, Merge_idx\")\n",
    "for iter_n, iteration in enumerate(iterable):\n",
    "    for splits_n, splits in enumerate(iteration):\n",
    "        for fold_n, fold in enumerate(splits):\n",
    "            print(iter_n, splits_n, fold_n, fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iter, Split, Merge_idx\n",
    "0 0 0 {'uid': '1', 'session': 1, 'run': 1}\n",
    "0 1 0 {'uid': '2', 'session': 1, 'run': 1}\n",
    "1 0 0 {'uid': '3', 'session': 1, 'run': 1}\n",
    "1 1 0 {'uid': '4', 'session': 1, 'run': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82df469",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = group_iterator(\n",
    "    [\n",
    "        dict(uid=[\"1\", \"2\"]),\n",
    "        dict(session=[1, 2]),\n",
    "        dict(run=[1, 2])\n",
    "    ]\n",
    ")\n",
    "# a = list(iterable)\n",
    "# c = [[list(x) for x in b] for b in a]\n",
    "for iter_n, iteration in enumerate(iterable):\n",
    "    for splits_n, splits in enumerate(iteration):\n",
    "        for fold_n, fold in enumerate(splits):\n",
    "            print(iter_n, splits_n, fold_n, fold)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "8071fe1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uid': 1, 'session': 1},\n",
       " {'uid': 1, 'session': 2},\n",
       " {'uid': 2, 'session': 1},\n",
       " {'uid': 2, 'session': 2}]"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(product_dict(uid=[1, 2], session=[1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "da8c2e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uid': '1', 'session': 1, 'run': 1},\n",
       " {'uid': '1', 'session': 1, 'run': 2},\n",
       " {'uid': '1', 'session': 2, 'run': 1},\n",
       " {'uid': '1', 'session': 2, 'run': 2},\n",
       " {'uid': '2', 'session': 1, 'run': 1},\n",
       " {'uid': '2', 'session': 1, 'run': 2},\n",
       " {'uid': '2', 'session': 2, 'run': 1},\n",
       " {'uid': '2', 'session': 2, 'run': 2}]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import GeneratorType\n",
    "from itertools import chain\n",
    "\n",
    "# def cv_product_dict(**kwargs):\n",
    "#     keys = kwargs.keys()\n",
    "#     vals = kwargs.values()\n",
    "#     for instance in product(*vals):\n",
    "#         yield dict(zip(keys, instance))\n",
    "\n",
    "# def group_iterator(split_kwargs_dicts, d=None, level=None):\n",
    "    \n",
    "#     level = level or 0\n",
    "# #     level = level or len(split_kwargs_dicts) - 1\n",
    "#     d = d or dict()\n",
    "\n",
    "\n",
    "#     for inner_d in cv_product_dict(**split_kwargs_dicts[level]): \n",
    "\n",
    "# #         if level == 0:\n",
    "#         if level == (len(split_kwargs_dicts) - 1):\n",
    "\n",
    "#             yield {**d, **inner_d}\n",
    "#         else:\n",
    "\n",
    "# #             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level + 1)\n",
    "\n",
    "\n",
    "ITERABLES_TYPES = (list, tuple, GeneratorType, product)\n",
    "\n",
    "def unpack_deep_iterable(deep_iterable):\n",
    "    # Keep levels as a nested list\n",
    "    if isinstance(deep_iterable, (GeneratorType, tuple, list)):\n",
    "        # If deep_iterable is iterable, just make sure that if it is a generator that it is iterated\n",
    "        deep_iterable = list(deep_iterable)\n",
    "        return [\n",
    "            unpack_deep_iterable(shallow_iterable)\n",
    "            for shallow_iterable in deep_iterable\n",
    "        ]\n",
    "    else:\n",
    "        return deep_iterable\n",
    "\n",
    "def flatten_deep_iterable(deep_iterable):\n",
    "    # Returns a flat iterator of all items that are not in ITERABLES_TYPES inside deep_iterable\n",
    "    for item in deep_iterable:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            for nested_item in flatten_deep_iterable(item):\n",
    "                yield nested_item\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "# def flatten_deep_iterable(container):\n",
    "#     return list(_flatten_deep_iterable(container))\n",
    "            \n",
    "\n",
    "    \n",
    "iterable = group_iterator(\n",
    "    [\n",
    "        dict(uid=[\"1\", \"2\"]),\n",
    "        dict(session=[1, 2]),\n",
    "        dict(run=[1, 2]),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "list(flatten_deep_iterable(iterable))\n",
    "# count_deep_iterable_levels(iterable, return_levels=True)\n",
    "# count_deep_iterable_levels([[1], [[2,], [[3, ], [4, 5, 6]]]], return_levels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d99ca5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_deep_iterable_levels(deep_iterable, level=0, return_levels=False):\n",
    "    # If return_levels, returns a nested list of the deep_iterable level\n",
    "    # Else returns the max level of the iterable\n",
    "    \n",
    "    if isinstance(deep_iterable, (GeneratorType, list, tuple)):\n",
    "        deep_iterable = list(deep_iterable)\n",
    "        levels = [count_deep_iterable_levels(shallow_iterable, level + 1, return_levels=return_levels) for shallow_iterable in deep_iterable]\n",
    "        if return_levels:\n",
    "            return levels\n",
    "        else:\n",
    "            if len(levels) == 0:\n",
    "                return level\n",
    "            return max(levels)\n",
    "    else:\n",
    "        return level\n",
    "    \n",
    "iterable = [\n",
    "    [1],\n",
    "    [\n",
    "        [1], [1], []\n",
    "    ]\n",
    "]\n",
    "count_deep_iterable_levels(\n",
    "    iterable, return_levels=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "ef298d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}],\n",
       "  [{'uid': 0}, {'uid': 1}]],\n",
       " [[{'uid': 0}, {'uid': 1}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}],\n",
       "  [{'uid': 2}, {'uid': 3}]],\n",
       " [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 6}, {'uid': 7}],\n",
       "  [{'uid': 4}, {'uid': 5}]],\n",
       " [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}],\n",
       "  [{'uid': 6}, {'uid': 7}]]]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def check_constraints(constraints, idx, kwargs):\n",
    "    constraints = {**constraints[idx]}\n",
    "    for kwarg in constraints:\n",
    "        print(kwarg)\n",
    "        if kwargs[kwarg] not in constraints[kwarg]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def constrained_group_iterator(split_kwargs_dicts, d=None, level=None, constraining_function=None, level_idx_dict=None):\n",
    "    constraining_function = constraining_function or (lambda l, i, kwargs: (kwargs, True))\n",
    "    level = level or 0\n",
    "    level_idx_dict = level_idx_dict or dict()\n",
    "#     level = level or len(split_kwargs_dicts) - 1\n",
    "    d = d or dict()\n",
    "\n",
    "    for idx, inner_d in enumerate(product_dict(**split_kwargs_dicts[level])):\n",
    "        level_idx_dict[level] = idx\n",
    "\n",
    "#         if level == 0:\n",
    "        kwargs = {**d, **inner_d}\n",
    "        kwargs, valid = constraining_function(level, level_idx_dict, kwargs)\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "            yield kwargs\n",
    "        else:\n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "            yield constrained_group_iterator(split_kwargs_dicts, d=kwargs, level=level + 1, constraining_function=constraining_function, level_idx_dict=level_idx_dict)\n",
    "\n",
    "def constrained_split_group_iterator(split_kwargs_dicts):\n",
    "\n",
    "    for iteration_splits_kwargs in group_iterator(split_kwargs_dicts):\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "\n",
    "def create_splitter_constraint_fn(splitter, uids):\n",
    "    split_df = pd.DataFrame()\n",
    "    for fold, (train_idx, test_idx) in enumerate(splitter.split(uids)):\n",
    "        train_uids, test_uids = uids[train_idx], uids[test_idx]\n",
    "        \n",
    "        train_split_df = pd.DataFrame()\n",
    "        train_split_df[\"uid\"] = train_uids\n",
    "        train_split_df[\"fold\"] = fold\n",
    "        train_split_df[\"group\"] = \"train\"\n",
    "        test_split_df = pd.DataFrame()\n",
    "        test_split_df[\"uid\"] = test_uids\n",
    "        test_split_df[\"fold\"] = fold\n",
    "        test_split_df[\"group\"] = \"test\"\n",
    "        split_df = pd.concat(\n",
    "            [\n",
    "                split_df,\n",
    "                pd.concat([train_split_df, test_split_df], axis=0),\n",
    "            ],\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "    def my_constraint_fn(level, level_idx_dict, kwargs):\n",
    "        kwargs = {**kwargs}\n",
    "\n",
    "        if (not \"group\" in kwargs) or (not \"uid\" in kwargs):\n",
    "            return kwargs, True\n",
    "\n",
    "        fold = kwargs[\"fold\"]\n",
    "        group = kwargs[\"group\"]\n",
    "        uid = kwargs[\"uid\"]\n",
    "#         r = uid in split_df.query(\"group == @group\").query(\"fold == @fold\").uid.to_numpy()\n",
    "        r = uid in split_df[(split_df.group == group) & (split_df.fold == fold)].uid.to_numpy()\n",
    "        kwargs.pop(\"group\")\n",
    "        kwargs.pop(\"fold\")\n",
    "\n",
    "        return kwargs, r\n",
    "    \n",
    "    return my_constraint_fn\n",
    "\n",
    "uids = np.arange(8)\n",
    "k = 4\n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(fold=np.arange(k)),\n",
    "        dict(group=[\"train\", \"test\"]),\n",
    "        dict(uid=uids),\n",
    "    ],\n",
    "    constraining_function=create_splitter_constraint_fn(KFold(k), uids)\n",
    ")\n",
    "list(unpack_deep_iterable(kfold_iterable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "2cdc25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"Split({'uid': 3},{'uid': 4},{'uid': 5},{'uid': 6},{'uid': 7},{'uid': 8})\",\n",
      "        \"Split({'uid': 0},{'uid': 1},{'uid': 2})\"\n",
      "    ],\n",
      "    [\n",
      "        \"Split({'uid': 0},{'uid': 1},{'uid': 2},{'uid': 5},{'uid': 6},{'uid': 7},{'uid': 8})\",\n",
      "        \"Split({'uid': 3},{'uid': 4})\"\n",
      "    ],\n",
      "    [\n",
      "        \"Split({'uid': 0},{'uid': 1},{'uid': 2},{'uid': 3},{'uid': 4},{'uid': 7},{'uid': 8})\",\n",
      "        \"Split({'uid': 5},{'uid': 6})\"\n",
      "    ],\n",
      "    [\n",
      "        \"Split({'uid': 0},{'uid': 1},{'uid': 2},{'uid': 3},{'uid': 4},{'uid': 5},{'uid': 6})\",\n",
      "        \"Split({'uid': 7},{'uid': 8})\"\n",
      "    ],\n",
      "    [\n",
      "        \"Split()\",\n",
      "        \"Split()\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def kfold_split_group_iterator(splitter, uids=None, k=5):\n",
    "    uids = np.arange(9)\n",
    "    kfold_iterable = constrained_group_iterator(\n",
    "        [\n",
    "            dict(fold=np.arange(k)),\n",
    "            dict(group=[\"train\", \"test\"]),\n",
    "            dict(uid=uids),\n",
    "        ],\n",
    "        constraining_function=create_splitter_constraint_fn(splitter, uids)\n",
    "    )\n",
    "    for iteration_splits_kwargs in kfold_iterable:\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "print(\n",
    "    json.dumps(\n",
    "        list(unpack_deep_iterable(kfold_split_group_iterator(KFold(4)))),\n",
    "        default=str,\n",
    "        indent=4\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "932ce677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6 7]\n",
      "[0 1]\n",
      "\n",
      "[0 1 4 5 6 7]\n",
      "[2 3]\n",
      "\n",
      "[0 1 2 3 6 7]\n",
      "[4 5]\n",
      "\n",
      "[0 1 2 3 4 5 7]\n",
      "[6]\n",
      "\n",
      "[0 1 2 3 4 5 6]\n",
      "[7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for train, test in KFold(5).split(np.arange(8)):\n",
    "    print(train)\n",
    "    print(test)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7938b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_constrained_group_iterator(split_kwargs_dicts, d, level, constraining_function, level_idx_dict):\n",
    "\n",
    "    for idx, inner_d in enumerate(product_dict(**split_kwargs_dicts[level])):\n",
    "        level_idx_dict[level] = idx\n",
    "\n",
    "#         if level == 0:\n",
    "        kwargs = {**d, **inner_d}\n",
    "        kwargs, valid = constraining_function(level, level_idx_dict, kwargs)\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "            yield kwargs\n",
    "        else:\n",
    "            \n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "            yield constrained_group_iterator(split_kwargs_dicts, d=kwargs, level=level + 1, constraining_function=constraining_function, level_idx_dict=level_idx_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "15272e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "uids = np.arange(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "17115bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327 ms ± 17.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "unpack_deep_iterable(\n",
    "    constrained_group_iterator(\n",
    "        [\n",
    "            dict(fold=np.arange(k)),\n",
    "            dict(group=[\"train\", \"test\"]),\n",
    "            dict(uid=uids),\n",
    "        ],\n",
    "        constraining_function=create_splitter_constraint_fn(KFold(k), uids)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c0c09333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308 ms ± 17 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "unpack_deep_iterable(\n",
    "    optimized_constrained_group_iterator(\n",
    "        [\n",
    "            dict(fold=np.arange(k)),\n",
    "            dict(group=[\"train\", \"test\"]),\n",
    "            dict(uid=uids),\n",
    "        ],\n",
    "        dict(),\n",
    "        0,\n",
    "        create_splitter_constraint_fn(KFold(k), uids),\n",
    "        dict()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "51e7e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[{'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}], [{'uid': 0}, {'uid': 1}]], [[{'uid': 0}, {'uid': 1}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}], [{'uid': 2}, {'uid': 3}]], [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 6}, {'uid': 7}], [{'uid': 4}, {'uid': 5}]], [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}], [{'uid': 6}, {'uid': 7}]], [[], []]]\n",
      "[[[{'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}], [{'uid': 0}, {'uid': 1}]], [[{'uid': 0}, {'uid': 1}, {'uid': 4}, {'uid': 5}, {'uid': 6}, {'uid': 7}], [{'uid': 2}, {'uid': 3}]], [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 6}, {'uid': 7}], [{'uid': 4}, {'uid': 5}]], [[{'uid': 0}, {'uid': 1}, {'uid': 2}, {'uid': 3}, {'uid': 4}, {'uid': 5}], [{'uid': 6}, {'uid': 7}]]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy, copy\n",
    "\n",
    "uids = np.arange(8)\n",
    "k = 4\n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(fold=np.arange(k + 1)),\n",
    "        dict(group=[\"train\", \"test\"]),\n",
    "        dict(uid=uids),\n",
    "    ],\n",
    "    constraining_function=create_splitter_constraint_fn(KFold(k), uids)\n",
    ")\n",
    "print(list(unpack_deep_iterable(kfold_iterable)))\n",
    "\n",
    "def is_deeply_empty(deep_list):\n",
    "    if not isinstance(deep_list, (GeneratorType, tuple, list)):\n",
    "        return False\n",
    "    \n",
    "    # If deep list is a generator, iterate and unpack it\n",
    "    deep_list = list(deep_list)\n",
    "    \n",
    "    # If it is empty, it is obviously deeply_empty\n",
    "    if len(deep_list) == 0:\n",
    "        return True\n",
    "            \n",
    "    # If not, it can only be deeply empty if all inner iterables are deeply_empty\n",
    "    is_empty_list = [is_deeply_empty(shallow_list) for shallow_list in deep_list]\n",
    "    \n",
    "    return all(is_empty_list)\n",
    "\n",
    "def prune_nested_list(deep_list):\n",
    "    # Keep levels as a nested list\n",
    "    if isinstance(deep_list, (GeneratorType, tuple, list)):\n",
    "        # If deep_iterable is iterable, just make sure that if it is a generator that it is iterated\n",
    "        deep_list = list(deep_list)\n",
    "        lists =  [\n",
    "            prune_nested_list(shallow_list)\n",
    "            for shallow_list in deep_list\n",
    "            if not is_deeply_empty(shallow_list)\n",
    "        ]\n",
    "        return lists\n",
    "    else:\n",
    "        return deep_list\n",
    "    \n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(fold=np.arange(k + 1)),\n",
    "        dict(group=[\"train\", \"test\"]),\n",
    "        dict(uid=uids),\n",
    "    ],\n",
    "    constraining_function=create_splitter_constraint_fn(KFold(k), uids)\n",
    ")\n",
    "# \n",
    "print(prune_nested_list(unpack_deep_iterable(kfold_iterable)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae780725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 0}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 1}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 2}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 3}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 4}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 6},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 5}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 7}],\n",
       "  [{'uid': 6}]],\n",
       " [[{'uid': 0},\n",
       "   {'uid': 1},\n",
       "   {'uid': 2},\n",
       "   {'uid': 3},\n",
       "   {'uid': 4},\n",
       "   {'uid': 5},\n",
       "   {'uid': 6}],\n",
       "  [{'uid': 7}]],\n",
       " [[], []]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uids = np.arange(8)\n",
    "k = 4\n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(fold=np.arange(9)),\n",
    "        dict(group=[\"train\", \"test\"]),\n",
    "        dict(uid=uids),\n",
    "    ],\n",
    "    constraining_function=create_kfold_constraint_fn(LeaveOneOut(), uids)\n",
    ")\n",
    "list(unpack_deep_iterable(kfold_iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "1cf27017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Split({'uid': 1, 'session': 1, 'run': 1}),\n",
       "  Split({'uid': 1, 'session': 1, 'run': 2})],\n",
       " [Split({'uid': 1, 'session': 2, 'run': 1}),\n",
       "  Split({'uid': 1, 'session': 2, 'run': 2})],\n",
       " [Split({'uid': 2, 'session': 1, 'run': 1}),\n",
       "  Split({'uid': 2, 'session': 1, 'run': 2})],\n",
       " [Split({'uid': 2, 'session': 2, 'run': 1}),\n",
       "  Split({'uid': 2, 'session': 2, 'run': 2})]]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_group_iterator(split_kwargs_dicts):\n",
    "\n",
    "    for iteration_splits_kwargs in group_iterator(split_kwargs_dicts):\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "        \n",
    "def create_split_group_iterator(outer_split_kwargs=None, inner_split_kwargs=None, merge_kwargs=None):\n",
    "    outer_split_kwargs = outer_split_kwargs or dict()\n",
    "    inner_split_kwargs = inner_split_kwargs or dict()\n",
    "    merge_kwargs = merge_kwargs or dict()\n",
    "    \n",
    "    return split_group_iterator(\n",
    "        [\n",
    "            outer_split_kwargs,\n",
    "            inner_split_kwargs,\n",
    "            merge_kwargs ,        \n",
    "        ]\n",
    "    ) \n",
    "inter_run_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1,], session=[1, 2]),\n",
    "    dict(),\n",
    "    dict(run=[1, 2]),        \n",
    ")\n",
    "intra_run_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1,], session=[1, 2], run=[1, 2]),\n",
    "    dict(),\n",
    "    dict(),\n",
    ")\n",
    "inter_session_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1,]),\n",
    "    dict(session=[1, 2]),\n",
    "    dict(run=[1, 2]),\n",
    ")\n",
    "inter_session_inter_run_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1,], run=[1, 2]),\n",
    "    dict(session=[1, 2]),\n",
    "    dict(),\n",
    ")\n",
    "intra_session_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1, 2], session=[1, 2]),\n",
    "    dict(),\n",
    "    dict(run=[1, 2]),\n",
    ")\n",
    "intra_session_inter_run_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1, 2], session=[1, 2]),\n",
    "    dict(run=[1, 2]),\n",
    "    dict(),\n",
    ")\n",
    "inter_subject = create_split_group_iterator(\n",
    "    dict(),\n",
    "    dict(uid=[1, 2, 3]),\n",
    "    dict(run=[1, 2], session=[1, 2]),\n",
    ")\n",
    "\n",
    "\n",
    "list(unpack_deep_iterable(inter_run_iterator))\n",
    "list(unpack_deep_iterable(intra_run_iterator))\n",
    "list(unpack_deep_iterable(inter_session_iterator))\n",
    "# list(unpack_deep_iterable(inter_session_inter_run_iterator))\n",
    "list(unpack_deep_iterable(intra_session_iterator))\n",
    "# list(unpack_deep_iterable(inter_run_iterator))\n",
    "list(unpack_deep_iterable(intra_session_inter_run_iterator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3284da6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session': 1, 'run': 1}"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_key(dict(uid=1, session=1, run=1), \"uid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7232ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/1388501995.py:15: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Epochs |  200 events (all good), -0.3 - 0.7 sec, baseline off, ~94.8 MB, data loaded,\n",
      " '0': 100\n",
      " '1': 100>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/1388501995.py:15: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Epochs |  200 events (all good), -0.3 - 0.7 sec, baseline off, ~94.8 MB, data loaded,\n",
      " '0': 100\n",
      " '1': 100>\n"
     ]
    }
   ],
   "source": [
    "class Split():\n",
    "    \n",
    "    def __init__(self, kwarg_dict_list, fold_sizes=None):\n",
    "        self.kwargs_list = kwarg_dict_list\n",
    "        self.fold_sizes = fold_sizes\n",
    "    \n",
    "    def to_dataframe():\n",
    "        return pd.DataFrame.from_records(self.kwargs_list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        dict_reps = [str(d) for d in self.kwargs_list]\n",
    "        return \"Split({})\".format(\",\".join(dict_reps))\n",
    "    \n",
    "    def load_epochs(self, dataset, **load_kwargs):\n",
    "        epochs = mne.concatenate_epochs(\n",
    "            [\n",
    "                dataset.load_subject(kwargs[\"uid\"], **remove_key(kwargs, \"uid\"), **load_kwargs)[0]\n",
    "                for kwargs in self.kwargs_list\n",
    "            ]\n",
    "        )\n",
    "        return epochs\n",
    "\n",
    "\n",
    "    \n",
    "inter_session_iterator = create_split_group_iterator(\n",
    "    dict(uid=['1', '2']),\n",
    "    dict(session=[1, 2]),\n",
    "    dict(run=[1, 2],),\n",
    ")\n",
    "for fold_splits in inter_session_iterator:\n",
    "    train_split, test_split = fold_splits\n",
    "    print(train_split.load_epochs(openbmi_dataset, reject=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3a346414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: path, dtype: object)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid = 1\n",
    "session = 1\n",
    "run = 1\n",
    "filepaths_df = openbmi_dataset.list_subject_filepaths()\n",
    "filepath = (\n",
    "    filepaths_df.query(\"uid == @uid\")\n",
    "    .query(\"run == @run\")\n",
    "    .query(\"session == @session\")\n",
    "    .path\n",
    ")\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "66048599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Split({'uid': 1, 'session': 1, 'run': 1},{'uid': 1, 'session': 1, 'run': 2})],\n",
       " [Split({'uid': 1, 'session': 2, 'run': 1},{'uid': 1, 'session': 2, 'run': 2})],\n",
       " [Split({'uid': 2, 'session': 1, 'run': 1},{'uid': 2, 'session': 1, 'run': 2})],\n",
       " [Split({'uid': 2, 'session': 2, 'run': 1},{'uid': 2, 'session': 2, 'run': 2})]]"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_from_processor(processor_fn):\n",
    "    \n",
    "    def transformer(*args, **kwargs):\n",
    "        item = processor_fn(*args, **kwargs)\n",
    "        if len(item)\n",
    "\n",
    "def myfn(item):\n",
    "    if isinstance(item, ITERABLES_TYPES):\n",
    "        if count_deep_iterable_levels(item) == 2:\n",
    "            return item[0]\n",
    "    return item\n",
    "\n",
    "def apply_to_iterator(iterator, fn, level=2):\n",
    "    for item in iterator:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            if count_deep_iterable_levels(item) == level:\n",
    "                \n",
    "                yield fn(item)\n",
    "            else:\n",
    "                yield apply_to_iterator(item, fn, level=level)\n",
    "        else:\n",
    "            yield fn(item)\n",
    "intra_session_iterator = create_split_group_iterator(\n",
    "    dict(uid=[1, 2], session=[1, 2]),\n",
    "    dict(),\n",
    "    dict(run=[1, 2]),\n",
    ")\n",
    "# count_deep_iterable_levels(intra_session_iterator)\n",
    "list(\n",
    "    unpack_deep_iterable(\n",
    "        apply_to_iterator(\n",
    "            intra_session_iterator,\n",
    "            myfn,\n",
    "#             lambda x: x,\n",
    "            level=2\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f069784a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9cf9b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 9, [9, 49]], [9, [9, 16]]]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def apply_to_iterator(iterator, fn):\n",
    "    for item in iterator:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            yield apply_to_iterator(item, fn)\n",
    "        else:\n",
    "            yield fn(item)\n",
    "\n",
    "iterator = [\n",
    "    [\n",
    "        1, 2, 3,\n",
    "        [\n",
    "            3, 7\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        3,\n",
    "        [\n",
    "            3, 4\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(\n",
    "    list(\n",
    "        unpack_deep_iterable(\n",
    "            apply_to_iterator(iterator, lambda x: x ** 2)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac07dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6acdaed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, [2, 3]]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1]]\n",
    "unpack_deep_iterable(chain(*[[1], [2, 3], [[2, 3]]]))\n",
    "# count_deep_iterable_levels(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1c60235a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.product_dict() argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8531/2424378512.py\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_deep_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8531/1469656023.py\u001b[0m in \u001b[0;36munpack_deep_iterable\u001b[0;34m(deep_iterable)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# If deep_iterable is iterable, just make sure that if it is a generator that it is iterated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mdeep_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         return [\n\u001b[1;32m     89\u001b[0m             \u001b[0munpack_deep_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshallow_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/3446916199.py\u001b[0m in \u001b[0;36mgroup_iterator\u001b[0;34m(outer_split_kwargs, inner_split_kwargs, merge_kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmerge_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0moutside_kwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mouter_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         splits = [\n\u001b[1;32m     19\u001b[0m             [\n",
      "\u001b[0;31mTypeError\u001b[0m: __main__.product_dict() argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "\n",
    "iterator_1 = group_iterator(\n",
    "    [\n",
    "        dict(uid=[1,]),\n",
    "        dict(session=[1, 2]),\n",
    "        dict(run=[1, 2]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "iterator_2 = group_iterator(\n",
    "    [\n",
    "        dict(uid=[1,]),\n",
    "        dict(session=[1, 2]),\n",
    "        dict(run=[1, 2]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "list(unpack_deep_iterable(iterator_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6743a561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "list(permutations([1, 2, 3], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5c63c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': (1, 2), 'b': (3, 4)},\n",
       " {'a': (1, 2), 'b': (4, 3)},\n",
       " {'a': (2, 1), 'b': (3, 4)},\n",
       " {'a': (2, 1), 'b': (4, 3)}]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def product_dict(**kwargs):\n",
    "#     keys = kwargs.keys()\n",
    "#     vals = kwargs.values()\n",
    "#     for instance in product(*vals):\n",
    "#         yield dict(zip(keys, instance))\n",
    "\n",
    "def combinatorial_product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    vals_combinations = [permutations(val) for val in vals]\n",
    "    for vals_combination in product(*vals_combinations):\n",
    "        for instance in vals_combination:\n",
    "            yield dict(zip(keys, instance))\n",
    "\n",
    "list(\n",
    "    product_dict(\n",
    "        a=permutations([1, 2]),\n",
    "        b=permutations([3, 4])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "67e232a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, [2], 4, [5], 3]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def concatenate_deep_iterators(*iterators):\n",
    "    iterators = [item if isinstance(item, ITERABLES_TYPES) else [item] for item in iterators]\n",
    "    \n",
    "    chained_iterators = chain(*iterators)\n",
    "    return chained_iterators\n",
    "list(concatenate_deep_iterators([1, [2]], [[4], [5]], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "4581c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal [\n",
      "    [\n",
      "        [\n",
      "            {\n",
      "                \"uid\": 1,\n",
      "                \"session\": 1,\n",
      "                \"run\": 1\n",
      "            },\n",
      "            {\n",
      "                \"uid\": 1,\n",
      "                \"session\": 1,\n",
      "                \"run\": 2\n",
      "            }\n",
      "        ],\n",
      "        [\n",
      "            {\n",
      "                \"uid\": 1,\n",
      "                \"session\": 2,\n",
      "                \"run\": 1\n",
      "            },\n",
      "            {\n",
      "                \"uid\": 1,\n",
      "                \"session\": 2,\n",
      "                \"run\": 2\n",
      "            }\n",
      "        ]\n",
      "    ]\n",
      "]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8531/2029814874.py\u001b[0m in \u001b[0;36m<cell line: 82>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     json.dumps(\n\u001b[1;32m     85\u001b[0m         list(\n\u001b[0;32m---> 86\u001b[0;31m             unpack_deep_iterable(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 combinatorial_group_iterator(\n\u001b[1;32m     88\u001b[0m                     [\n",
      "\u001b[0;32m/tmp/ipykernel_8531/2833980289.py\u001b[0m in \u001b[0;36munpack_deep_iterable\u001b[0;34m(deep_iterable)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# If deep_iterable is iterable, just make sure that if it is a generator that it is iterated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdeep_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         return [\n\u001b[1;32m     37\u001b[0m             \u001b[0munpack_deep_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshallow_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/2029814874.py\u001b[0m in \u001b[0;36mcombinatorial_group_iterator\u001b[0;34m(split_kwargs_dicts, make_combinations, d, level)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mnew_make_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 permutation_dict = {\n\u001b[0m\u001b[1;32m     45\u001b[0m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmake_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8531/2029814874.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 permutation_dict = {\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpermutations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmake_combinations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0msplit_kwargs_dicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'session'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "\n",
    "#Needs fix\n",
    "# def group_iterator(split_kwargs_dicts, d=None, level=None):\n",
    "    \n",
    "#     level = level or 0\n",
    "# #     level = level or len(split_kwargs_dicts) - 1\n",
    "#     d = d or dict()\n",
    "\n",
    "\n",
    "#     for inner_d in product_dict(**split_kwargs_dicts[level]): \n",
    "\n",
    "# #         if level == 0:\n",
    "#         if level == (len(split_kwargs_dicts) - 1):\n",
    "\n",
    "#             yield {**d, **inner_d}\n",
    "#         else:\n",
    "\n",
    "# #             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level + 1)\n",
    "\n",
    "\n",
    "def combinatorial_group_iterator(split_kwargs_dicts, make_combinations=None, d=None, level=None):\n",
    "    \n",
    "    level = level or 0\n",
    "#     level = level or len(split_kwargs_dicts) - 1\n",
    "    d = d or dict()\n",
    "    make_combinations = make_combinations or [dict() for level in split_kwargs_dicts]\n",
    "\n",
    "    for inner_d in product_dict(**split_kwargs_dicts[level]): \n",
    "\n",
    "#         if level == 0:\n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "\n",
    "            yield {**d, **inner_d}\n",
    "        else:\n",
    "            \n",
    "            if level < (len(split_kwargs_dicts) - 1):\n",
    "                new_make_combinations = copy(make_combinations)\n",
    "                new_make_combinations[level + 1] = False\n",
    "\n",
    "                permutation_dict = {\n",
    "                    k: permutations(v) if make_combinations[level][k] else [v]\n",
    "                    for k, v\n",
    "                    in split_kwargs_dicts[level + 1].items()\n",
    "                }\n",
    "\n",
    "                iterators = list()\n",
    "                for permuted_d in product_dict(**permutation_dict):\n",
    "                    new_split_kwargs_dicts = deepcopy(split_kwargs_dicts)\n",
    "                    new_split_kwargs_dicts[level + 1] = permuted_d\n",
    "                    iterator = combinatorial_group_iterator(new_split_kwargs_dicts, new_make_combinations, d={**d, **inner_d}, level=level + 1)\n",
    "                    iterators.append(iterator)\n",
    "        #             return iterator\n",
    "                yield list(chain(*iterators))\n",
    "            else:\n",
    "\n",
    "#             yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level-1)\n",
    "                yield combinatorial_group_iterator(split_kwargs_dicts, make_combinations, d={**d, **inner_d}, level=level + 1)\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Normal\",\n",
    "    json.dumps(\n",
    "        list(\n",
    "            unpack_deep_iterable(\n",
    "                group_iterator(\n",
    "                    [\n",
    "                        dict(uid=[1,]),\n",
    "                        dict(session=[1, 2]),\n",
    "                        dict(run=[1, 2]),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        indent=4\n",
    "    )\n",
    ")\n",
    "        \n",
    "print(\n",
    "    \"Combinatorial\",\n",
    "    json.dumps(\n",
    "        list(\n",
    "            unpack_deep_iterable(\n",
    "                combinatorial_group_iterator(\n",
    "                    [\n",
    "                        dict(uid=[1,]),\n",
    "                        dict(session=[1, 2]),\n",
    "                        dict(run=[1, 2]),\n",
    "                    ],\n",
    "                    make_combinations=[\n",
    "                        dict(uid=False),\n",
    "                        dict(session=True),\n",
    "                        dict(run=False)\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        ),\n",
    "        indent=4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8237f2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 3},\n",
       " {'a': 1, 'b': 4},\n",
       " {'a': 2, 'b': 3},\n",
       " {'a': 2, 'b': 4},\n",
       " {'a': 3, 'b': 3},\n",
       " {'a': 3, 'b': 4},\n",
       " {'a': 3, 'b': 1},\n",
       " {'a': 3, 'b': 2},\n",
       " {'a': 3, 'b': 3},\n",
       " {'a': 4, 'b': 1},\n",
       " {'a': 4, 'b': 2},\n",
       " {'a': 4, 'b': 3}]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\n",
    "    combinatorial_product_dict(\n",
    "        a=[1, 2, 3],\n",
    "        b=[3, 4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5073198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Epochs |  120 events (all good), -0.3 - 0.7 sec, baseline off, ~56.9 MB, data loaded,\n",
       "  '0': 63\n",
       "  '1': 57>,\n",
       " <Epochs |  120 events (all good), -0.3 - 0.7 sec, baseline off, ~56.9 MB, data loaded,\n",
       "  '0': 63\n",
       "  '1': 57>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_epochs_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad5b7f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108    /home/paulo/Documents/datasets/OpenBMI/edf/ses...\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths_df = openbmi_dataset.list_subject_filepaths()\n",
    "uid = '25'\n",
    "session = 1\n",
    "train = True\n",
    "(\n",
    "    filepaths_df.query(\"uid == @uid\")\n",
    "    .query(\"train == @train\")\n",
    "    .query(\"session == @session\")\n",
    "    .path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43d4510c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>train</th>\n",
       "      <th>session</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>/home/paulo/Documents/datasets/OpenBMI/edf/ses...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path  train  session uid\n",
       "108  /home/paulo/Documents/datasets/OpenBMI/edf/ses...   True        1  25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths_df.query(\"uid == @uid\").query(\"train == @train\").query(\"session == @session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93c0610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '1', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '1', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.5903271692745377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '2', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '2', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.8466749866286325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '3', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '3', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.8866999465145302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '4', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '4', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.47759601706970123\n"
     ]
    }
   ],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class GetData(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        filter_kwargs = dict(\n",
    "            method=\"iir\",\n",
    "            iir_params=dict(\n",
    "                order=5,\n",
    "                ftype=\"butter\"\n",
    "            )\n",
    "        )\n",
    "        return X.load_data().filter(8, 30, **filter_kwargs).get_data()\n",
    "\n",
    "\n",
    "# kwargs = dict(\n",
    "#     uids=[\"1\", \"2\", \"3\", \"4\"],\n",
    "#     inter_session=False,\n",
    "#     inter_subject=True,\n",
    "#     sessions=[2],\n",
    "#     train_folds=[True, False],\n",
    "#     fold_sizes=[.5, .5],\n",
    "#     load_kwargs=dict(\n",
    "#         reject=False\n",
    "#     )\n",
    "# )\n",
    "kwargs = dict(\n",
    "    uids=[\"1\", \"2\", \"3\", \"4\"],\n",
    "    inter_session=False,\n",
    "    inter_subject=False,\n",
    "    sessions=[1],\n",
    "    train_folds=[True, False],\n",
    "    fold_sizes=[.25, .75],\n",
    "    load_kwargs=dict(\n",
    "        tmin=1,\n",
    "        tmax=3.5,\n",
    "        reject=False\n",
    "    )\n",
    ")\n",
    "for j, splits in enumerate(OpenBMI_Splitter(openbmi_dataset).make_splits(**kwargs)):\n",
    "    infos = [info for info, split in splits]\n",
    "    splits = [split for info, split in splits]\n",
    "    print(infos)\n",
    "    clf = make_pipeline(\n",
    "        GetData(),\n",
    "        CSP(5, log=True),\n",
    "        LDA()\n",
    "    )\n",
    "    train_epochs = splits[0]\n",
    "    train_labels = train_epochs.events[:, 2]\n",
    "    \n",
    "    test_epochs = splits[1]\n",
    "    test_labels = test_epochs.events[:, 2]\n",
    "    \n",
    "    clf.fit(train_epochs, train_labels)\n",
    "    pred = clf.predict(test_epochs)\n",
    "    acc = balanced_accuracy_score(test_labels, pred)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7e4a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1effa6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 0', '0 1', '0 2', '1 0', '1 1', '1 2', '2 0', '2 1', '2 2']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{i} {j}\" for i in range(3) for j in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb4f5277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 10, 40]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_array(arr, n_splits=2, sizes=None):\n",
    "    n = len(arr)\n",
    "    sizes = sizes or [1 / n] * n_splits\n",
    "    assert np.sum(sizes) == 1.\n",
    "    sizes = [int(size * n) for size in sizes]\n",
    "    sizes = [0] + sizes\n",
    "    sizes = np.cumsum(sizes)\n",
    "    slices = [slice(start, end) for start, end in zip(sizes[:-1], sizes[1:])]\n",
    "    arrs = [arr[s] for s in slices]\n",
    "    return arrs\n",
    "\n",
    "arrs = split_array(np.random.rand(100), sizes=[.2, .3, .1, .4])\n",
    "list(map(len, arrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd82d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['25', '15', '41', '12', '37', '2', '42', '6', '52', '18'],\n",
       "       dtype=object),\n",
       " array(['38', '30', '34', '5', '11', '44', '29', '8', '17', '33', '46',\n",
       "        '23', '19', '4', '13', '22'], dtype=object),\n",
       " array(['35', '45', '36', '39', '16', '50', '10', '53', '7', '28', '27',\n",
       "        '14', '32', '20', '31', '51', '24', '9', '21', '40', '47', '26',\n",
       "        '49', '54', '43', '1', '3'], dtype=object)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_array(openbmi_dataset.list_uids(), sizes=[.2, .3, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "287a069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from warnings import warn\n",
    "from itertools import product, chain\n",
    "import numpy as np\n",
    "from mne import Epochs\n",
    "import mne\n",
    "from types import GeneratorType\n",
    "from ica_benchmark.io.load import OpenBMI_Dataset\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "ITERABLES_TYPES = (list, tuple, GeneratorType, product, chain)\n",
    "\n",
    "\n",
    "def apply_to_iterator(iterator, fn):\n",
    "    for item in iterator:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            yield apply_to_iterator(item, fn)\n",
    "        else:\n",
    "            yield fn(item)\n",
    "\n",
    "\n",
    "def unpack_deep_iterable(deep_iterable):\n",
    "    # Keep levels as a nested list\n",
    "    if isinstance(deep_iterable, (GeneratorType, tuple, list)):\n",
    "        # If deep_iterable is iterable, just make sure that if it is a generator that it is iterated\n",
    "        deep_iterable = list(deep_iterable)\n",
    "        return [\n",
    "            unpack_deep_iterable(shallow_iterable)\n",
    "            for shallow_iterable in deep_iterable\n",
    "        ]\n",
    "    else:\n",
    "        return deep_iterable\n",
    "\n",
    "\n",
    "def flatten_deep_iterable(deep_iterable):\n",
    "    # Returns a flat iterator of all items that are not in ITERABLES_TYPES inside deep_iterable\n",
    "    for item in deep_iterable:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            for nested_item in flatten_deep_iterable(item):\n",
    "                yield nested_item\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def apply_to_iterator(iterator, fn):\n",
    "    for item in iterator:\n",
    "        if isinstance(item, ITERABLES_TYPES):\n",
    "            yield apply_to_iterator(item, fn)\n",
    "        else:\n",
    "            yield fn(item)\n",
    "\n",
    "\n",
    "def make_epochs_splits_indexes(arr, n=None, n_splits=2, sizes=None, shuffle=False, seed=1):\n",
    "    if not isinstance(arr, (Epochs,)):\n",
    "        arr = np.array(arr)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    if n is None:\n",
    "        if isinstance(arr, Epochs):\n",
    "            n = len(arr.events)\n",
    "        else:\n",
    "            n = len(arr)\n",
    "\n",
    "    sizes = sizes or [1 / n_splits] * n_splits\n",
    "\n",
    "    assert np.sum(sizes) == 1.\n",
    "\n",
    "    sizes = np.cumsum(\n",
    "        [0] + [int(size * n) for size in sizes]\n",
    "    )\n",
    "\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    slices = [slice(start, end) for start, end in zip(sizes[:-1], sizes[1:])]\n",
    "    indexes = [idx[s] for s in slices]\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def make_epochs_splits(arr, n=None, n_splits=2, sizes=None, shuffle=False, seed=1):\n",
    "    indexes = make_epochs_splits_indexes(arr, n=n, n_splits=n_splits, sizes=sizes, shuffle=shuffle, seed=seed)\n",
    "    arrs = [arr[idx] for idx in indexes]\n",
    "    return arrs\n",
    "\n",
    "\n",
    "def remove_key(d, k):\n",
    "    return {\n",
    "        key: value\n",
    "        for key, value in d.items()\n",
    "        if key != k\n",
    "    }\n",
    "\n",
    "\n",
    "def product_dict(**kwargs):\n",
    "    keys = kwargs.keys()\n",
    "    vals = kwargs.values()\n",
    "    for instance in product(*vals):\n",
    "        yield dict(zip(keys, instance))\n",
    "\n",
    "\n",
    "def insideout_group_iterator(split_kwargs_dicts, d=None, level=None):\n",
    "\n",
    "    level = level or len(split_kwargs_dicts) - 1\n",
    "\n",
    "    for inner_d in product_dict(**split_kwargs_dicts[level]):\n",
    "        if level == 0:\n",
    "            yield {**d, **inner_d}\n",
    "        else:\n",
    "            yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level - 1)\n",
    "\n",
    "\n",
    "def group_iterator(split_kwargs_dicts, d=None, level=None):\n",
    "\n",
    "    level = level or 0\n",
    "    d = d or dict()\n",
    "\n",
    "    for inner_d in product_dict(**split_kwargs_dicts[level]): \n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "            yield {**d, **inner_d}\n",
    "        else:\n",
    "            yield group_iterator(split_kwargs_dicts, d={**d, **inner_d}, level=level + 1)\n",
    "\n",
    "\n",
    "def split_group_iterator(split_kwargs_dicts):\n",
    "\n",
    "    for iteration_splits_kwargs in group_iterator(split_kwargs_dicts):\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "\n",
    "\n",
    "def create_split_group_iterator(outer_split_kwargs=None, inner_split_kwargs=None, merge_kwargs=None):\n",
    "    outer_split_kwargs = outer_split_kwargs or dict()\n",
    "    inner_split_kwargs = inner_split_kwargs or dict()\n",
    "    merge_kwargs = merge_kwargs or dict()\n",
    "\n",
    "    return split_group_iterator(\n",
    "        [\n",
    "            outer_split_kwargs,\n",
    "            inner_split_kwargs,\n",
    "            merge_kwargs,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class Split():\n",
    "\n",
    "    def __init__(self, kwarg_dict_list):\n",
    "        self.kwargs_list = kwarg_dict_list\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        return pd.DataFrame.from_records(self.kwargs_list)\n",
    "\n",
    "    def __repr__(self):\n",
    "        dict_reps = [str(d) for d in self.kwargs_list]\n",
    "        return \"Split({})\".format(\",\".join(dict_reps))\n",
    "\n",
    "    def load_epochs(self, dataset, **load_kwargs):\n",
    "        epochs = mne.concatenate_epochs(\n",
    "            [\n",
    "                dataset.load_subject(kwargs[\"uid\"], **remove_key(kwargs, \"uid\"), **load_kwargs)[0]\n",
    "                for kwargs in self.kwargs_list\n",
    "            ]\n",
    "        )\n",
    "        return epochs\n",
    "\n",
    "\n",
    "def constrained_group_iterator(split_kwargs_dicts, d=None, level=None, constraining_function=None, level_idx_dict=None):\n",
    "    constraining_function = constraining_function or (lambda l, i, kwargs: (kwargs, True))\n",
    "    level = level or 0\n",
    "    level_idx_dict = level_idx_dict or dict()\n",
    "    d = d or dict()\n",
    "\n",
    "    for idx, inner_d in enumerate(product_dict(**split_kwargs_dicts[level])):\n",
    "        level_idx_dict[level] = idx\n",
    "\n",
    "        kwargs = {**d, **inner_d}\n",
    "        kwargs, valid = constraining_function(level, level_idx_dict, kwargs)\n",
    "        if not valid:\n",
    "            continue\n",
    "\n",
    "        if level == (len(split_kwargs_dicts) - 1):\n",
    "            yield kwargs\n",
    "        else:\n",
    "            yield constrained_group_iterator(\n",
    "                split_kwargs_dicts,\n",
    "                d=kwargs,\n",
    "                level=level + 1,\n",
    "                constraining_function=constraining_function,\n",
    "                level_idx_dict=level_idx_dict\n",
    "            )\n",
    "\n",
    "\n",
    "def constrained_split_group_iterator(split_kwargs_dicts):\n",
    "\n",
    "    for iteration_splits_kwargs in group_iterator(split_kwargs_dicts):\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "\n",
    "\n",
    "def create_splitter_constraint_fn(splitter, uids):\n",
    "\n",
    "    # Creates a dataframe with columns fold, uid and group (train or test)\n",
    "    # It is only used to later check if volunteer with uid is in train or test for each fold.\n",
    "    splits_dfs = list()\n",
    "    for fold, splits_idx in enumerate(splitter.split(uids)):\n",
    "        for group_i, group_uid_idx in enumerate(splits_idx):\n",
    "            split_uids = uids[group_uid_idx]\n",
    "            split_df = pd.DataFrame()\n",
    "            split_df[\"uid\"] = split_uids\n",
    "            split_df[\"fold\"] = fold\n",
    "            split_df[\"group\"] = group_i\n",
    "            splits_dfs.append(split_df)\n",
    "\n",
    "    split_df = pd.concat(splits_dfs, axis=0)\n",
    "\n",
    "    def my_constraint_fn(level, level_idx_dict, kwargs):\n",
    "        kwargs = {**kwargs}\n",
    "\n",
    "        if (\"group\" not in kwargs) or (\"uid\" not in kwargs):\n",
    "            return kwargs, True\n",
    "\n",
    "        r = (\n",
    "            kwargs[\"uid\"] in \n",
    "            split_df[(split_df.group == kwargs[\"group\"]) & (split_df.fold == kwargs[\"fold\"])].uid.to_numpy()\n",
    "        )\n",
    "        kwargs.pop(\"group\")\n",
    "        kwargs.pop(\"fold\")\n",
    "\n",
    "        return kwargs, r\n",
    "\n",
    "    return my_constraint_fn\n",
    "\n",
    "\n",
    "def kfold_split_group_iterator(splitter, uids, n_groups=2):\n",
    "    \n",
    "    kfold_iterable = constrained_group_iterator(\n",
    "        [\n",
    "            dict(fold=np.arange(splitter.get_n_splits())),\n",
    "            dict(group=np.arange(n_groups)),\n",
    "            dict(uid=uids),\n",
    "        ],\n",
    "        constraining_function=create_splitter_constraint_fn(splitter, uids)\n",
    "    )\n",
    "    for iteration_splits_kwargs in kfold_iterable:\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "\n",
    "\n",
    "class Splitter():\n",
    "\n",
    "    SESSION_KWARGS = dict(intra=dict(), inter=dict())\n",
    "\n",
    "    def default_splitter(self):\n",
    "        splitter = KFold(4)\n",
    "        warn(\"Using default splitter: \" + str(splitter))\n",
    "        return splitter\n",
    "\n",
    "    def __init__(self, dataset, uids, sessions, runs, load_kwargs=None, splitter=None, unsafe=False, intra_session_shuffle=False, fold_sizes=None):\n",
    "        self.dataset = dataset\n",
    "        self.uids = uids\n",
    "        self.sessions = sessions\n",
    "        self.runs = runs\n",
    "        self.load_kwargs = load_kwargs or load_kwargs\n",
    "        self.splitter = splitter or self.default_splitter()\n",
    "        self.intra_session_shuffle = intra_session_shuffle\n",
    "        self.fold_sizes = fold_sizes\n",
    "\n",
    "    def validate_config(self, mode):\n",
    "        valid_modes = [\n",
    "            \"inter_subject\",\n",
    "            \"inter_session\",\n",
    "            \"intra_session_intra_run\",\n",
    "            \"intra_session_inter_run\",\n",
    "            \"intra_session_intra_run_merged\"\n",
    "        ]\n",
    "        fold_sizes = self.fold_sizes\n",
    "        assert mode in valid_modes, \"Please choose one mode among the following: {}\".format(\", \".join(valid_modes))\n",
    "        if mode == \"inter_subject\":\n",
    "            if fold_sizes is not None:\n",
    "                warn(\"You are using the inter_subject mode, so the fold_sizes argument will not be used\")\n",
    "        elif mode == \"inter_session\":\n",
    "            if len(self.runs) > 1:\n",
    "                warn(\"You are using inter session protocol with more than one run. All runs from each session will be concatenated and yielded in different steps.\")\n",
    "        elif (mode == \"intra_session_inter_run\"):\n",
    "            if (len(self.runs) == 1):\n",
    "                warn(\"You are using an intra session protocol, splitting by run, but only passed one run. The splitter can only yield one epoch at time (from the only run you passed as argument)\")\n",
    "        elif mode in (\"intra_session_intra_run\", \"intra_session_intra_run_merged\"):\n",
    "            if fold_sizes is None:\n",
    "                warn(\"You are using intra session intra run protocol with no fold sizes. The splitter will only yield one epoch at time\")\n",
    "\n",
    "    def kfold_split_group_iterator(self, n_groups=2):\n",
    "    \n",
    "        kfold_iterable = constrained_group_iterator(\n",
    "            [\n",
    "                dict(\n",
    "                    fold=np.arange(\n",
    "                        self.splitter.get_n_splits()\n",
    "                    )\n",
    "                ),\n",
    "                dict(group=np.arange(n_groups)),\n",
    "                dict(\n",
    "                    uid=self.uids,\n",
    "                    session=self.sessions,\n",
    "                    run=self.runs\n",
    "                ),\n",
    "            ],\n",
    "            constraining_function=create_splitter_constraint_fn(self.splitter, uids)\n",
    "        )\n",
    "        for iteration_splits_kwargs in kfold_iterable:\n",
    "            yield [\n",
    "                Split(\n",
    "                    [\n",
    "                        dict(\n",
    "                            **split_kwargs\n",
    "                        )\n",
    "                        for split_kwargs in splits_kwargs_list\n",
    "                    ]\n",
    "                )\n",
    "                for splits_kwargs_list in iteration_splits_kwargs\n",
    "            ]\n",
    "                \n",
    "    def inter_subject(self, splitter=None):\n",
    "        return self.kfold_split_group_iterator()\n",
    "\n",
    "    def inter_session(self):\n",
    "        inter_session_iterator = create_split_group_iterator(\n",
    "            dict(uid=self.uids),\n",
    "            dict(session=self.sessions),\n",
    "            dict(run=self.runs),\n",
    "        )\n",
    "        return inter_session_iterator\n",
    "\n",
    "    def intra_session_inter_run(self):\n",
    "        intra_session_inter_run_iterator = create_split_group_iterator(\n",
    "            dict(uid=self.uids, session=self.sessions),\n",
    "            dict(run=self.runs),\n",
    "            dict(),\n",
    "        )\n",
    "        return intra_session_inter_run_iterator\n",
    "\n",
    "    def intra_session_intra_run(self, merge=False):\n",
    "        # intra_session_intra_run_merge\n",
    "        # Duas runs mergidas, precisa separar por pct\n",
    "        if merge:\n",
    "            return self.intra_session_intra_run_merged()\n",
    "        else:\n",
    "            # Cada run em seu experiment, mas ainda precisa separar por pct\n",
    "            intra_run_iterator = create_split_group_iterator(\n",
    "                dict(uid=self.uids, session=self.sessions, run=self.runs),\n",
    "                dict(),\n",
    "                dict(),\n",
    "            )\n",
    "            return intra_run_iterator\n",
    "\n",
    "    def intra_session_intra_run_merged(self, merge=False):\n",
    "        intra_session_iterator = create_split_group_iterator(\n",
    "            dict(uid=self.uids, session=self.sessions),\n",
    "            dict(),\n",
    "            dict(run=self.runs),\n",
    "        )\n",
    "        return intra_session_iterator\n",
    "\n",
    "    def yield_splits_epochs(self, mode):\n",
    "\n",
    "        split_fn_dict = dict(\n",
    "            # Intra subject, inter session\n",
    "            inter_session=self.inter_session,\n",
    "            # Inter subject, will concatenate all sessions and runs\n",
    "            inter_subject=self.inter_subject,\n",
    "            # Intra subject, intra_session, inter run (will split runs)\n",
    "            intra_session_inter_run=self.intra_session_inter_run,\n",
    "            # Intra subject, intra_session, intra run (will split using fold sizes)\n",
    "            intra_session_intra_run=self.intra_session_intra_run,\n",
    "            # Intra subject, intra_session, intra run (will merge all runs and split using fold sizes)\n",
    "            intra_session_intra_run_merged=self.intra_session_intra_run_merged,\n",
    "        )\n",
    "\n",
    "        split_fn = split_fn_dict[mode]\n",
    "        for fold_splits in split_fn():\n",
    "\n",
    "            # if (len(fold_splits) == 1) and (fold_sizes is None):\n",
    "            #     warn(\"This splitter return only one split and you passed no fold sizes for intra splitting. Is this what you want?\")\n",
    "                \n",
    "            yield fold_splits\n",
    "\n",
    "    def load_from_split(self, splits, fold_sizes=None):\n",
    "        fold_sizes = fold_sizes or self.fold_sizes\n",
    "        splits_epochs = [\n",
    "            split.load_epochs(self.dataset, **self.load_kwargs)\n",
    "            for split in splits\n",
    "        ]\n",
    "        print(fold_sizes)\n",
    "        if fold_sizes is not None:\n",
    "            assert len(splits_epochs) == 1, \"You passed fold_sizes={} but there in more than one split\".format(fold_sizes)\n",
    "            splits_epochs = make_epochs_splits(\n",
    "                splits_epochs[0],\n",
    "                sizes=fold_sizes,\n",
    "                shuffle=self.intra_session_shuffle\n",
    "            )\n",
    "        return splits_epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "a54c8b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/2884798730.py:24: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  kwargs[\"uid\"] in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[], []], [[], []], [[], []], [[], []]]"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = KFold(4)\n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(\n",
    "            fold=np.arange(\n",
    "                s.get_n_splits()\n",
    "            )\n",
    "        ),\n",
    "        dict(group=np.arange(2)),\n",
    "        dict(\n",
    "            uid=np.array([\"1\", \"2\", \"3\", \"4\"]),\n",
    "            session=[1, 2],\n",
    "            run=[1, 2]\n",
    "        ),\n",
    "    ],\n",
    "    constraining_function=create_splitter_constraint_fn(s, uids)\n",
    ")\n",
    "list(unpack_deep_iterable(kfold_iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "c3beb4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'uid': '2', 'session': 1, 'run': 1},\n",
       "   {'uid': '2', 'session': 1, 'run': 2},\n",
       "   {'uid': '2', 'session': 2, 'run': 1},\n",
       "   {'uid': '2', 'session': 2, 'run': 2},\n",
       "   {'uid': '3', 'session': 1, 'run': 1},\n",
       "   {'uid': '3', 'session': 1, 'run': 2},\n",
       "   {'uid': '3', 'session': 2, 'run': 1},\n",
       "   {'uid': '3', 'session': 2, 'run': 2},\n",
       "   {'uid': '4', 'session': 1, 'run': 1},\n",
       "   {'uid': '4', 'session': 1, 'run': 2},\n",
       "   {'uid': '4', 'session': 2, 'run': 1},\n",
       "   {'uid': '4', 'session': 2, 'run': 2}],\n",
       "  [{'uid': '1', 'session': 1, 'run': 1},\n",
       "   {'uid': '1', 'session': 1, 'run': 2},\n",
       "   {'uid': '1', 'session': 2, 'run': 1},\n",
       "   {'uid': '1', 'session': 2, 'run': 2}]],\n",
       " [[{'uid': '1', 'session': 1, 'run': 1},\n",
       "   {'uid': '1', 'session': 1, 'run': 2},\n",
       "   {'uid': '1', 'session': 2, 'run': 1},\n",
       "   {'uid': '1', 'session': 2, 'run': 2},\n",
       "   {'uid': '3', 'session': 1, 'run': 1},\n",
       "   {'uid': '3', 'session': 1, 'run': 2},\n",
       "   {'uid': '3', 'session': 2, 'run': 1},\n",
       "   {'uid': '3', 'session': 2, 'run': 2},\n",
       "   {'uid': '4', 'session': 1, 'run': 1},\n",
       "   {'uid': '4', 'session': 1, 'run': 2},\n",
       "   {'uid': '4', 'session': 2, 'run': 1},\n",
       "   {'uid': '4', 'session': 2, 'run': 2}],\n",
       "  [{'uid': '2', 'session': 1, 'run': 1},\n",
       "   {'uid': '2', 'session': 1, 'run': 2},\n",
       "   {'uid': '2', 'session': 2, 'run': 1},\n",
       "   {'uid': '2', 'session': 2, 'run': 2}]],\n",
       " [[{'uid': '1', 'session': 1, 'run': 1},\n",
       "   {'uid': '1', 'session': 1, 'run': 2},\n",
       "   {'uid': '1', 'session': 2, 'run': 1},\n",
       "   {'uid': '1', 'session': 2, 'run': 2},\n",
       "   {'uid': '2', 'session': 1, 'run': 1},\n",
       "   {'uid': '2', 'session': 1, 'run': 2},\n",
       "   {'uid': '2', 'session': 2, 'run': 1},\n",
       "   {'uid': '2', 'session': 2, 'run': 2},\n",
       "   {'uid': '4', 'session': 1, 'run': 1},\n",
       "   {'uid': '4', 'session': 1, 'run': 2},\n",
       "   {'uid': '4', 'session': 2, 'run': 1},\n",
       "   {'uid': '4', 'session': 2, 'run': 2}],\n",
       "  [{'uid': '3', 'session': 1, 'run': 1},\n",
       "   {'uid': '3', 'session': 1, 'run': 2},\n",
       "   {'uid': '3', 'session': 2, 'run': 1},\n",
       "   {'uid': '3', 'session': 2, 'run': 2}]],\n",
       " [[{'uid': '1', 'session': 1, 'run': 1},\n",
       "   {'uid': '1', 'session': 1, 'run': 2},\n",
       "   {'uid': '1', 'session': 2, 'run': 1},\n",
       "   {'uid': '1', 'session': 2, 'run': 2},\n",
       "   {'uid': '2', 'session': 1, 'run': 1},\n",
       "   {'uid': '2', 'session': 1, 'run': 2},\n",
       "   {'uid': '2', 'session': 2, 'run': 1},\n",
       "   {'uid': '2', 'session': 2, 'run': 2},\n",
       "   {'uid': '3', 'session': 1, 'run': 1},\n",
       "   {'uid': '3', 'session': 1, 'run': 2},\n",
       "   {'uid': '3', 'session': 2, 'run': 1},\n",
       "   {'uid': '3', 'session': 2, 'run': 2}],\n",
       "  [{'uid': '4', 'session': 1, 'run': 1},\n",
       "   {'uid': '4', 'session': 1, 'run': 2},\n",
       "   {'uid': '4', 'session': 2, 'run': 1},\n",
       "   {'uid': '4', 'session': 2, 'run': 2}]]]"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_splitter_constraint_fn(splitter, uids):\n",
    "\n",
    "    # Creates a dataframe with columns fold, uid and group (train or test)\n",
    "    # It is only used to later check if volunteer with uid is in train or test for each fold.\n",
    "    splits_dfs = list()\n",
    "    for fold, splits_idx in enumerate(splitter.split(uids)):\n",
    "        for group_i, group_uid_idx in enumerate(splits_idx):\n",
    "            split_uids = uids[group_uid_idx]\n",
    "            split_df = pd.DataFrame()\n",
    "            split_df[\"uid\"] = split_uids\n",
    "            split_df[\"fold\"] = fold\n",
    "            split_df[\"group\"] = group_i\n",
    "            splits_dfs.append(split_df)\n",
    "\n",
    "    split_df = pd.concat(splits_dfs, axis=0)\n",
    "\n",
    "    def my_constraint_fn(level, level_idx_dict, kwargs):\n",
    "        kwargs = {**kwargs}\n",
    "\n",
    "        if (\"group\" not in kwargs) or (\"uid\" not in kwargs):\n",
    "            return kwargs, True\n",
    "\n",
    "        r = (\n",
    "            kwargs[\"uid\"] in \n",
    "            split_df[(split_df.group == kwargs[\"group\"]) & (split_df.fold == kwargs[\"fold\"])].uid.to_numpy()\n",
    "        )\n",
    "        kwargs.pop(\"group\")\n",
    "        kwargs.pop(\"fold\")\n",
    "\n",
    "        return kwargs, r\n",
    "\n",
    "    return my_constraint_fn\n",
    "s = KFold(4)\n",
    "f = create_splitter_constraint_fn(s, np.array(np.array([\"1\", \"2\", \"3\", \"4\"])))\n",
    "uids = np.array([\"1\", \"2\", \"3\", \"4\"])\n",
    "kfold_iterable = constrained_group_iterator(\n",
    "    [\n",
    "        dict(\n",
    "            fold=np.arange(\n",
    "                s.get_n_splits()\n",
    "            )\n",
    "        ),\n",
    "        dict(group=np.arange(2)),\n",
    "        dict(\n",
    "            uid=uids,\n",
    "            session=[1, 2],\n",
    "            run=[1, 2]\n",
    "        ),\n",
    "    ],\n",
    "    constraining_function=create_splitter_constraint_fn(s, uids)\n",
    ")\n",
    "list(unpack_deep_iterable(kfold_iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "e4c54824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "\tSplits [Split(), Split()]\n",
      "Fold 1\n",
      "\tSplits [Split(), Split()]\n",
      "Fold 2\n",
      "\tSplits [Split(), Split()]\n",
      "Fold 3\n",
      "\tSplits [Split(), Split()]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8531/3362983087.py:242: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  kwargs[\"uid\"] in\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openbmi_dataset_folderpath = Path('/home/paulo/Documents/datasets/OpenBMI/edf/')\n",
    "dataset = OpenBMI_Dataset(openbmi_dataset_folderpath)\n",
    "fold_sizes = None\n",
    "splitter = Splitter(\n",
    "    dataset,\n",
    "    uids=dataset.list_uids()[:4],\n",
    "    sessions=dataset.SESSIONS,\n",
    "    runs=dataset.RUNS,\n",
    "    load_kwargs=dict(\n",
    "        reject=False\n",
    "    ),\n",
    "    splitter=KFold(4),\n",
    "    intra_session_shuffle=False,\n",
    "    fold_sizes=fold_sizes\n",
    ")\n",
    "\n",
    "splits_iterable = splitter.yield_splits_epochs(mode=\"inter_subject\")\n",
    "for i, fold_splits in enumerate(splits_iterable):\n",
    "    print(f\"Fold {i}\")\n",
    "    print(f\"\\tSplits {fold_splits}\")\n",
    "#     epochs = splitter.load_from_split(fold_splits, fold_sizes=fold_sizes)\n",
    "#     print(f\"\\tEpochs {epochs}\")\n",
    "#     print()\n",
    "#     del epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "1d786a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter.sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "1d454875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Split({'uid': 2},{'uid': 3},{'uid': 4}), Split({'uid': 1})],\n",
       " [Split({'uid': 1},{'uid': 3},{'uid': 4}), Split({'uid': 2})],\n",
       " [Split({'uid': 1},{'uid': 2},{'uid': 4}), Split({'uid': 3})],\n",
       " [Split({'uid': 1},{'uid': 2},{'uid': 3}), Split({'uid': 4})]]"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_splitter_constraint_fn(splitter, uids):\n",
    "    \n",
    "    # Creates a dataframe with columns fold, uid and group (train or test)\n",
    "    # It is only used to later check if volunteer with uid is in train or test for each fold.\n",
    "    splits_dfs = list()\n",
    "    for fold, splits_idx in enumerate(splitter.split(uids)):\n",
    "        for group_i, group_uid_idx in enumerate(splits_idx):\n",
    "            split_uids = uids[group_uid_idx]\n",
    "            split_df = pd.DataFrame()\n",
    "            split_df[\"uid\"] = split_uids\n",
    "            split_df[\"fold\"] = fold\n",
    "            split_df[\"group\"] = group_i\n",
    "            splits_dfs.append(split_df)\n",
    "            \n",
    "    split_df = pd.concat(splits_dfs, axis=0)\n",
    "    def my_constraint_fn(level, level_idx_dict, kwargs):\n",
    "        kwargs = {**kwargs}\n",
    "\n",
    "        if (\"group\" not in kwargs) or (\"uid\" not in kwargs):\n",
    "            return kwargs, True\n",
    "\n",
    "        r = (\n",
    "            kwargs[\"uid\"] in \n",
    "            split_df[(split_df.group == kwargs[\"group\"]) & (split_df.fold == kwargs[\"fold\"])].uid.to_numpy()\n",
    "        )\n",
    "        kwargs.pop(\"group\")\n",
    "        kwargs.pop(\"fold\")\n",
    "\n",
    "        return kwargs, r\n",
    "\n",
    "    return my_constraint_fn\n",
    "\n",
    "\n",
    "def kfold_split_group_iterator(splitter, uids, n_groups=2):\n",
    "    \n",
    "    kfold_iterable = constrained_group_iterator(\n",
    "        [\n",
    "            dict(fold=np.arange(splitter.get_n_splits())),\n",
    "            dict(group=np.arange(n_groups)),\n",
    "            dict(uid=uids),\n",
    "        ],\n",
    "        constraining_function=create_splitter_constraint_fn(splitter, uids)\n",
    "    )\n",
    "    for iteration_splits_kwargs in kfold_iterable:\n",
    "        yield [\n",
    "            Split(\n",
    "                [\n",
    "                    dict(\n",
    "                        **split_kwargs\n",
    "                    )\n",
    "                    for split_kwargs in splits_kwargs_list\n",
    "                ]\n",
    "            )\n",
    "            for splits_kwargs_list in iteration_splits_kwargs\n",
    "        ]\n",
    "\n",
    "list(unpack_deep_iterable(kfold_split_group_iterator(KFold(4), np.array([1, 2, 3, 4]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c51ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
