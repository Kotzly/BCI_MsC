{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922240c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ica_benchmark.io.load import Physionet_2009_Dataset, BCI_IV_Comp_Dataset, OpenBMI_Dataset\n",
    "import mne\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "\n",
    "mne.set_log_level(False)\n",
    "\n",
    "physionet_dataset_folderpath = Path('/home/paulo/Documents/datasets/Physionet')\n",
    "bci_dataset_folderpath = Path('/home/paulo/Documents/datasets/BCI_Comp_IV_2a/gdf/')\n",
    "bci_test_dataset_folderpath = Path('/home/paulo/Documents/datasets/BCI_Comp_IV_2a/true_labels/')\n",
    "openbmi_dataset_folderpath = Path('/home/paulo/Documents/datasets/OpenBMI/edf/')\n",
    "\n",
    "physionet_dataset = Physionet_2009_Dataset(physionet_dataset_folderpath)\n",
    "bci_dataset = BCI_IV_Comp_Dataset(bci_dataset_folderpath, test_folder=bci_test_dataset_folderpath)\n",
    "openbmi_dataset = OpenBMI_Dataset(openbmi_dataset_folderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87796cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epochs_splits(arr, n=None, n_splits=2, sizes=None, shuffle=False, seed=1):\n",
    "    if isinstance(arr, (tuple, list)):\n",
    "        arr = np.array(arr)\n",
    "    np.random.seed(seed)\n",
    "    n = n or len(arr)\n",
    "    sizes = sizes or [1 / n_splits] * n_splits\n",
    "\n",
    "    assert np.sum(sizes) == 1.\n",
    "    \n",
    "    sizes = np.cumsum(\n",
    "        [0] + [int(size * n) for size in sizes]\n",
    "    )\n",
    "    \n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    slices = [slice(start, end) for start, end in zip(sizes[:-1], sizes[1:])]\n",
    "    arrs = [arr[idx[s]] for s in slices]\n",
    "    return arrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16e4953d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9457/4001071399.py\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m#     fold_sizes=[.4, .6]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FOLD {j}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9457/4001071399.py\u001b[0m in \u001b[0;36mmake_splits\u001b[0;34m(self, inter_session, inter_subject)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintra_session_default_splitting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You are using the intra session protocol but passed more than 1 session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9457/4001071399.py\u001b[0m in \u001b[0;36mintra_session_default_splitting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Your are using an intra session splitting but had more than 1 session\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         splits = [\n\u001b[0m\u001b[1;32m     70\u001b[0m             (\n\u001b[1;32m     71\u001b[0m                 \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9457/4001071399.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m             (\n\u001b[1;32m     71\u001b[0m                 \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_subject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uid' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class Splitter():\n",
    "    \n",
    "    INTER_SESSION = True\n",
    "    INTER_SUBJECT = True\n",
    "    TRAIN_TEST = True\n",
    "    \n",
    "    UNIQUE_SESSION = False\n",
    "    \n",
    "    SESSION_KWARGS = dict(intra=dict(), inter=dict())\n",
    "    \n",
    "    def __init__(self, dataset, uids, sessions, train_folds, load_kwargs=None, fold_sizes=None):\n",
    "        self.dataset = dataset\n",
    "        self.uids = uids\n",
    "        self.sessions = sessions\n",
    "        self.train_folds = train_folds\n",
    "        self.load_kwargs = load_kwargs or load_kwargs\n",
    "        self.fold_sizes = fold_sizes\n",
    "\n",
    "    def inter_subject_splitting(self, shuffle=False, sizes=[.5, .5], seed=1):\n",
    "        np.random.seed(seed)\n",
    "        uids = deepcopy(self.uids)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(uids)\n",
    "        splits_uids = make_epochs_splits(uids, sizes=sizes)\n",
    "        # [(info, epochs), (info, epochs), ...]\n",
    "        splits = [\n",
    "            (\n",
    "                dict(\n",
    "                    sessions=self.sessions,\n",
    "                    train_folds=self.train_folds,\n",
    "                    uid=split_uids\n",
    "                ),\n",
    "                mne.concatenate_epochs(\n",
    "                    [\n",
    "                        self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                        for session in self.sessions\n",
    "                        for train in self.train_folds\n",
    "                        for uid in split_uids\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            for split_uids in splits_uids\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def inter_session_splitting(self):\n",
    "        assert len(self.sessions) > 1, \"You are using the inter session protocol, but only passed 1 session\"\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uids=self.uids, sessions=[session], train_folds=self.train_folds),\n",
    "                mne.concatenate_epochs(\n",
    "                    [\n",
    "                        self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                        for train\n",
    "                        in self.train_folds\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            for uid in self.uids\n",
    "            for session in self.sessions\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def intra_session_default_splitting(self):\n",
    "        \n",
    "        assert len(self.sessions) == 1, \"Your are using an intra session splitting but had more than 1 session\"\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uid=self.uids, sessions=[session], train_folds=[train]),\n",
    "                self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "            )\n",
    "            for session in self.sessions\n",
    "            for train in self.train_folds\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "    def intra_session_splitting(self, uid, session):\n",
    "        epochs = mne.concatenate_epochs(\n",
    "            [\n",
    "                self.dataset.load_subject(uid, session=session, train=train, **self.load_kwargs)[0]\n",
    "                for train in self.train_folds\n",
    "            ]\n",
    "        )\n",
    "        splits = make_epochs_splits(\n",
    "            epochs,\n",
    "            n=len(epochs.events),\n",
    "            sizes=self.fold_sizes\n",
    "        )\n",
    "        splits = [\n",
    "            (\n",
    "                dict(uid=self.uids, session=session, train_folds=self.train_folds, size=size),\n",
    "                split\n",
    "            )\n",
    "            for split, size\n",
    "            in zip(splits, self.fold_sizes)\n",
    "        ]\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def make_splits(self, inter_session=True, inter_subject=False):\n",
    "\n",
    "        assert all(np.isin(self.uids, self.dataset.list_uids()))\n",
    "        session_key = \"inter\" if inter_session else \"intra\"\n",
    "        subject_key = \"inter\" if inter_subject else \"intra\"\n",
    "        \n",
    "        splits = list()\n",
    "\n",
    "        # Inter session for the inter subject protocol does not make sense.\n",
    "        # So, if inter_subject is true, inter_session does not matter\n",
    "        if inter_subject:\n",
    "            splits = self.inter_subject_splitting()\n",
    "            yield splits\n",
    "            \n",
    "        # Intra subject\n",
    "        else:\n",
    "            \n",
    "            for uid in self.uids:\n",
    "                if inter_session:\n",
    "                    splits = self.inter_session_splitting()\n",
    "                    yield splits\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    if self.fold_sizes is None:\n",
    "                        splits = self.intra_session_default_splitting()\n",
    "                    else:\n",
    "                        assert len(self.sessions) == 1, \"You are using the intra session protocol but passed more than 1 session\"\n",
    "                        session = self.sessions[0]\n",
    "                        splits = self.intra_session_splitting(uid, session)\n",
    "                        \n",
    "                    yield splits\n",
    "import numpy as np\n",
    "kwargs = dict(\n",
    "    inter_session=False,\n",
    "    inter_subject=False\n",
    ")\n",
    "splitter = Splitter(\n",
    "    openbmi_dataset,\n",
    "    uids=[\"1\", \"2\"],\n",
    "    sessions=[1],\n",
    "    train_folds=[True, False],\n",
    "    load_kwargs=dict(\n",
    "        reject=False\n",
    "    )\n",
    "#     fold_sizes=[.4, .6]\n",
    ")\n",
    "for j, splits in enumerate(splitter.make_splits(**kwargs)):\n",
    "    print(f\"FOLD {j}\")\n",
    "    for i, (info, split) in enumerate(splits):\n",
    "        print(f\"Split {i}\")\n",
    "        print(\"\\tInfo\", info)\n",
    "        print(\"\\tSplit\", split)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4ccd73a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '1', 'session': 1, 'train': True}, {'uid': '1', 'session': 1, 'train': False}, {'uid': '1', 'session': 2, 'train': True}, {'uid': '1', 'session': 2, 'train': False}]\n",
      "\n",
      "[{'uid': '2', 'session': 1, 'train': True}, {'uid': '2', 'session': 1, 'train': False}, {'uid': '2', 'session': 2, 'train': True}, {'uid': '2', 'session': 2, 'train': False}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def group_iterator(*args):\n",
    "    split_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in args\n",
    "        if arg.to_split\n",
    "    ]\n",
    "    merge_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in args\n",
    "        if not arg.to_split\n",
    "    ]\n",
    "    # Split loop\n",
    "    for split_args in product(*[arg.arg_list for arg in split_args_iter]):\n",
    "                \n",
    "        split_arg_dict = {\n",
    "            arg.name: split_args[i]\n",
    "            for i, arg\n",
    "            in enumerate(split_args_iter)\n",
    "        }\n",
    "        # Merge loop\n",
    "        merge_args_list = list()\n",
    "        for merge_args in product(*[arg.arg_list for arg in merge_args_iter]):\n",
    "            merge_arg_dict = {\n",
    "                arg.name: merge_args[i]\n",
    "                for i, arg\n",
    "                in enumerate(merge_args_iter)\n",
    "            }\n",
    "            run_kwargs = {**split_arg_dict, **merge_arg_dict}\n",
    "            merge_args_list.append(run_kwargs)\n",
    "        yield merge_args_list\n",
    "\n",
    "\n",
    "load_kwargs=dict(\n",
    "    tmin=1,\n",
    "    tmax=3.5,\n",
    "    reject=False\n",
    ")\n",
    "\n",
    "from copy import deepcopy\n",
    "def split(dataset, uids, sessions, train_folds, fold_sizes, load_kwargs):\n",
    "    kwargs_iterator = group_iterator(\n",
    "        SplitArg(\"uid\", uids, True),\n",
    "        SplitArg(\"session\", sessions, False),\n",
    "        SplitArg(\"train\", train_folds, False),\n",
    "    )\n",
    "    epochs_list = list()\n",
    "    \n",
    "    for splits_kwargs in kwargs_iterator:\n",
    "        split_epochs = list()\n",
    "        for split_kwargs in splits_kwargs:\n",
    "            kwargs = deepcopy(split_kwargs)\n",
    "            uid = kwargs.pop(\"uid\")\n",
    "            epochs, _ = dataset.load_subject(uid, **kwargs, **load_kwargs)\n",
    "            split_epochs.append(epochs)\n",
    "        print(splits_kwargs)\n",
    "        print()\n",
    "        if fold_sizes is not None:\n",
    "            split_epochs = mne.concatenate_epochs(split_epochs)\n",
    "            split_epochs = make_epochs_splits(split_epochs, n=len(split_epochs.events), sizes=fold_sizes)\n",
    "        yield split_epochs\n",
    "#         yield splits_kwargs\n",
    "\n",
    "#         epochs_list.append(epochs)\n",
    "#     epochs = mne.concatenate_epochs(epochs_list)\n",
    "#     yield split\n",
    "for k in split(openbmi_dataset, [\"1\", \"2\"], [1, 2], [True, False], None, load_kwargs):\n",
    "#     print(k)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8ebcbe08",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13178/1980763300.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "k[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "af496a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f40bae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'session': 1, 'uid': 'A', 'train': True}\n",
      "{'session': 1, 'uid': 'A', 'train': False}\n",
      "{'session': 1, 'uid': 'B', 'train': True}\n",
      "{'session': 1, 'uid': 'B', 'train': False}\n",
      "{'session': 1, 'uid': 'C', 'train': True}\n",
      "{'session': 1, 'uid': 'C', 'train': False}\n",
      "\n",
      "{'session': 2, 'uid': 'A', 'train': True}\n",
      "{'session': 2, 'uid': 'A', 'train': False}\n",
      "{'session': 2, 'uid': 'B', 'train': True}\n",
      "{'session': 2, 'uid': 'B', 'train': False}\n",
      "{'session': 2, 'uid': 'C', 'train': True}\n",
      "{'session': 2, 'uid': 'C', 'train': False}\n",
      "\n",
      "{'session': 3, 'uid': 'A', 'train': True}\n",
      "{'session': 3, 'uid': 'A', 'train': False}\n",
      "{'session': 3, 'uid': 'B', 'train': True}\n",
      "{'session': 3, 'uid': 'B', 'train': False}\n",
      "{'session': 3, 'uid': 'C', 'train': True}\n",
      "{'session': 3, 'uid': 'C', 'train': False}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "    \n",
    "class SplitArg():\n",
    "    def __init__(self, name, arg_list, to_split):\n",
    "        self.to_split = to_split\n",
    "        self.arg_list = arg_list\n",
    "        self.name = name\n",
    "\n",
    "DEFAULT_FOLDS = [True, False]\n",
    "DEFAULT_SESSIONS = [1, 2]\n",
    "DEFAULT_UIDS = [\"1\"]\n",
    "\n",
    "def split(dataset, uids, sessions, train_folds, fold_sizes, load_kwargs):\n",
    "    \n",
    "    args = [uids, sessions, train_folds, fold_sizes]\n",
    "    split_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in [uids, sessions, train_folds]\n",
    "        if arg.to_split\n",
    "    ]\n",
    "    merge_args_iter = [\n",
    "        arg\n",
    "        for arg\n",
    "        in [uids, sessions, train_folds]\n",
    "        if not arg.to_split\n",
    "    ]\n",
    "    \n",
    "    data = dict()\n",
    "    \n",
    "    # Split loop\n",
    "    for split_args in product(*[arg.arg_list for arg in split_args_iter]):\n",
    "                \n",
    "        # Merge loop\n",
    "        split_arg_dict = {\n",
    "            arg.name: split_args[i]\n",
    "            for i, arg\n",
    "            in enumerate(split_args_iter)\n",
    "        }\n",
    "        epochs_list = list()\n",
    "        for merge_args in product(*[arg.arg_list for arg in merge_args_iter]):\n",
    "            merge_arg_dict = {\n",
    "                arg.name: merge_args[i]\n",
    "                for i, arg\n",
    "                in enumerate(merge_args_iter)\n",
    "            }\n",
    "            run_kwargs = {**split_arg_dict, **merge_arg_dict, **load_kwargs}\n",
    "            print(run_kwargs)\n",
    "#             epochs, _ = dataset.load_subject(**run_kwargs)\n",
    "#             epochs_list.append(epochs)\n",
    "        \n",
    "        if fold_sizes is not None:\n",
    "            epochs = mne.concatenate_epochs(epochs_list)\n",
    "            idx = np.arange(len(epochs))\n",
    "            np.random.\n",
    "\n",
    "        print()\n",
    "\n",
    "split(\n",
    "    None,\n",
    "    SplitArg(\"uid\", [\"A\", \"B\", \"C\"], False),\n",
    "    SplitArg(\"session\", [1, 2, 3], True),\n",
    "    SplitArg(\"train\", [True, False], False),\n",
    "    [.3, .2, .5],\n",
    "    dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdfb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93c0610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '1', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '1', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.5903271692745377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '2', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '2', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.8466749866286325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '3', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '3', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.8866999465145302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9466/3590559943.py:107: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  epochs = mne.concatenate_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'uid': '4', 'session': 1, 'train_folds': [True, False], 'size': 0.25}, {'uid': '4', 'session': 1, 'train_folds': [True, False], 'size': 0.75}]\n",
      "0.47759601706970123\n"
     ]
    }
   ],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "class GetData(BaseEstimator):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        filter_kwargs = dict(\n",
    "            method=\"iir\",\n",
    "            iir_params=dict(\n",
    "                order=5,\n",
    "                ftype=\"butter\"\n",
    "            )\n",
    "        )\n",
    "        return X.load_data().filter(8, 30, **filter_kwargs).get_data()\n",
    "\n",
    "\n",
    "# kwargs = dict(\n",
    "#     uids=[\"1\", \"2\", \"3\", \"4\"],\n",
    "#     inter_session=False,\n",
    "#     inter_subject=True,\n",
    "#     sessions=[2],\n",
    "#     train_folds=[True, False],\n",
    "#     fold_sizes=[.5, .5],\n",
    "#     load_kwargs=dict(\n",
    "#         reject=False\n",
    "#     )\n",
    "# )\n",
    "kwargs = dict(\n",
    "    uids=[\"1\", \"2\", \"3\", \"4\"],\n",
    "    inter_session=False,\n",
    "    inter_subject=False,\n",
    "    sessions=[1],\n",
    "    train_folds=[True, False],\n",
    "    fold_sizes=[.25, .75],\n",
    "    load_kwargs=dict(\n",
    "        tmin=1,\n",
    "        tmax=3.5,\n",
    "        reject=False\n",
    "    )\n",
    ")\n",
    "for j, splits in enumerate(OpenBMI_Splitter(openbmi_dataset).make_splits(**kwargs)):\n",
    "    infos = [info for info, split in splits]\n",
    "    splits = [split for info, split in splits]\n",
    "    print(infos)\n",
    "    clf = make_pipeline(\n",
    "        GetData(),\n",
    "        CSP(5, log=True),\n",
    "        LDA()\n",
    "    )\n",
    "    train_epochs = splits[0]\n",
    "    train_labels = train_epochs.events[:, 2]\n",
    "    \n",
    "    test_epochs = splits[1]\n",
    "    test_labels = test_epochs.events[:, 2]\n",
    "    \n",
    "    clf.fit(train_epochs, train_labels)\n",
    "    pred = clf.predict(test_epochs)\n",
    "    acc = balanced_accuracy_score(test_labels, pred)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a7e4a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1effa6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 0', '0 1', '0 2', '1 0', '1 1', '1 2', '2 0', '2 1', '2 2']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{i} {j}\" for i in range(3) for j in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb4f5277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 10, 40]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_array(arr, n_splits=2, sizes=None):\n",
    "    n = len(arr)\n",
    "    sizes = sizes or [1 / n] * n_splits\n",
    "    assert np.sum(sizes) == 1.\n",
    "    sizes = [int(size * n) for size in sizes]\n",
    "    sizes = [0] + sizes\n",
    "    sizes = np.cumsum(sizes)\n",
    "    slices = [slice(start, end) for start, end in zip(sizes[:-1], sizes[1:])]\n",
    "    arrs = [arr[s] for s in slices]\n",
    "    return arrs\n",
    "\n",
    "arrs = split_array(np.random.rand(100), sizes=[.2, .3, .1, .4])\n",
    "list(map(len, arrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd82d7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['25', '15', '41', '12', '37', '2', '42', '6', '52', '18'],\n",
       "       dtype=object),\n",
       " array(['38', '30', '34', '5', '11', '44', '29', '8', '17', '33', '46',\n",
       "        '23', '19', '4', '13', '22'], dtype=object),\n",
       " array(['35', '45', '36', '39', '16', '50', '10', '53', '7', '28', '27',\n",
       "        '14', '32', '20', '31', '51', '24', '9', '21', '40', '47', '26',\n",
       "        '49', '54', '43', '1', '3'], dtype=object)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_array(openbmi_dataset.list_uids(), sizes=[.2, .3, .5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "287a069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulo/anaconda3/envs/bci_update/lib/python3.9/site-packages/mne/io/edf/edf.py:1123: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "/home/paulo/anaconda3/envs/bci_update/lib/python3.9/contextlib.py:124: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n",
      "/home/paulo/anaconda3/envs/bci_update/lib/python3.9/site-packages/mne/io/edf/edf.py:1123: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  etmode = np.fromstring(etmode, UINT8).tolist()[0]\n",
      "/home/paulo/anaconda3/envs/bci_update/lib/python3.9/contextlib.py:124: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Epochs |  57 events (good & bad), -0.3 - 0.7 sec, baseline off, ~26 kB, data not loaded>,\n",
       " <Epochs |  230 events (good & bad), -0.3 - 0.7 sec, baseline off, ~26 kB, data not loaded>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = bci_dataset.load_subject(\"1\")[0]\n",
    "split_array(e, n=len(e.events), sizes=[.2, .8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
